import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import cv2
from tqdm import tqdm
import logging
from datetime import datetime
import json
import matplotlib.pyplot as plt
import random
import os
from scipy.ndimage import gaussian_filter, rotate, zoom, map_coordinates
from scipy import ndimage
import warnings
import re
import pandas as pd
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

warnings.filterwarnings('ignore')


# ==================== Global Configuration ====================

TRAIN_DIR = ""
VAL_DIR = ""
TEST_DIR1 = ""
TEST_DIR2 = ""
CLINICAL_FILE = ""
SAVE_DIR = ""
LOG_DIR = ""

RESUME_CHECKPOINT = None  
CONTINUE_TRAINING = False 
TRAINING_CURVES_FILE = os.path.join(SAVE_DIR, "training_curves.json") 

MODALITIES = ['T1', 'T2', 'T1C']
NUM_MODALITIES = len(MODALITIES)

BATCH_SIZE = 8              
MAX_ITERATIONS = 20000
INITIAL_LEARNING_RATE = 1e-4 
WEIGHT_DECAY = 1e-3       

GRADIENT_CLIP_VALUE = 1.0  
LABEL_SMOOTHING = 0.2  

LOAD_COMPLETE_MODEL = None      
LOAD_SEGMENTATION_MODEL = None   
LOAD_CLASSIFICATION_MODEL = None 

FREEZE_SEGMENTATION = False     
FREEZE_CLASSIFICATION = False  

SEGMENTATION_LR = 1e-4  
CLASSIFICATION_LR = 1e-5 

ENABLE_CLASSIFICATION = True

IMG_SIZE = (512, 512)  
OUT_CHANNELS = 1
BASE_FEATURES = 4    

VIS_INTERVAL = 200
SAVE_INTERVAL = 500
EVAL_INTERVAL = 200
NUM_WORKERS = 12
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")


# ==================== Modal Completion Components ====================

class LightweightModalEncoder(nn.Module):
    """
    Lightweight encoder for individual modality feature extraction.
    Processes single modality with 3 convolutional blocks.
    """
    
    def __init__(self, base_features):
        super(LightweightModalEncoder, self).__init__()
        
        self.conv1 = nn.Sequential(
            nn.Conv2d(1, base_features, 3, padding=1, bias=False),
            nn.BatchNorm2d(base_features),
            nn.ReLU(inplace=True)
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(base_features, base_features, 3, padding=1, bias=False),
            nn.BatchNorm2d(base_features),
            nn.ReLU(inplace=True)
        )
        self.conv3 = nn.Sequential(
            nn.Conv2d(base_features, base_features, 3, padding=1, bias=False),
            nn.BatchNorm2d(base_features),
            nn.ReLU(inplace=True)
        )
    
    def forward(self, x):
        """
        Args:
            x: [B, 1, H, W]
        Returns:
            [B, base_features, H, W]
        """
        x = self.conv1(x)
        x = self.conv2(x)
        x = self.conv3(x)
        return x


class CrossModalGenerator(nn.Module):
    """
    Generates pseudo features for missing modalities using available ones.
    Uses residual blocks and per-modality output heads.
    """
    
    def __init__(self, base_features, num_modalities=3):
        super(CrossModalGenerator, self).__init__()
        
        self.num_modalities = num_modalities
        
        self.generator_blocks = nn.Sequential(
            self._make_residual_block(base_features * num_modalities, base_features * 2),
            self._make_residual_block(base_features * 2, base_features * 2),
            self._make_residual_block(base_features * 2, base_features * num_modalities)
        )
        
        self.modal_heads = nn.ModuleDict({
            'T1': nn.Conv2d(base_features * num_modalities, base_features, 1),
            'T2': nn.Conv2d(base_features * num_modalities, base_features, 1),
            'T1C': nn.Conv2d(base_features * num_modalities, base_features, 1)
        })
    
    def _make_residual_block(self, in_channels, out_channels):
        """Create residual block with skip connection."""
        return nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, padding=1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, 3, padding=1, bias=False),
            nn.BatchNorm2d(out_channels)
        )
    
    def forward(self, modal_features, modal_mask):
        """
        Generate complete feature set for all modalities.
        
        Args:
            modal_features: dict {modality: tensor}
            modal_mask: [B, num_modalities] availability indicator
        Returns:
            complete_features: dict with generated features for missing modalities
        """
        B, device = modal_mask.shape[0], modal_mask.device
        modalities = ['T1', 'T2', 'T1C']
        
        sample_feat = next(iter(modal_features.values()))
        H, W = sample_feat.shape[2:]
        C = sample_feat.shape[1]
        
        # Stack available and missing features
        concat_features = []
        for i, mod in enumerate(modalities):
            if mod in modal_features:
                concat_features.append(modal_features[mod])
            else:
                concat_features.append(torch.zeros(B, C, H, W, device=device))
        
        concat_input = torch.cat(concat_features, dim=1)
        generated = self.generator_blocks(concat_input)

        complete_features = {}
        for i, mod in enumerate(modalities):
            if mod in modal_features:
                complete_features[mod] = modal_features[mod]  
            else:
                complete_features[mod] = self.modal_heads[mod](generated)
        
        return complete_features


class ModalCompletionLayer(nn.Module):
    """
    Complete multi-modal feature extraction with missing modality handling.
    Combines lightweight encoders and cross-modal generators.
    """
    
    def __init__(self, base_features, modalities):
        super(ModalCompletionLayer, self).__init__()
        
        self.modalities = modalities
        self.base_features = base_features
        
        self.modal_encoders = nn.ModuleDict({
            mod: LightweightModalEncoder(base_features) for mod in modalities
        })
        
        self.cross_modal_generator = CrossModalGenerator(base_features, len(modalities))
    
    def forward(self, images, modal_mask):
        """
        Process multi-modal images and complete missing modalities.
        
        Args:
            images: [B, num_modalities, H, W]
            modal_mask: [B, num_modalities] availability mask
        Returns:
            completed_tensor: [B, 3*base_features, H, W]
            complete_features: dict of completed feature maps
            reconstruction_targets: dict of ground truth features
        """
        B = images.shape[0]
        
        available_features = {}
        all_modal_features = {}
        
        # Encode each modality
        for i, mod in enumerate(self.modalities):
            modal_input = images[:, i:i+1]
            encoded = self.modal_encoders[mod](modal_input)
            all_modal_features[mod] = encoded
            
            if modal_mask[:, i].sum() > 0:
                available_features[mod] = encoded
        
        # Generate missing modalities if needed
        if len(available_features) < len(self.modalities):
            complete_features = self.cross_modal_generator(available_features, modal_mask)
        else:
            complete_features = available_features
        
        # Concatenate all modality features
        completed_tensor = torch.cat([
            complete_features[mod] for mod in self.modalities
        ], dim=1)
        
        reconstruction_targets = {
            mod: all_modal_features[mod] for mod in self.modalities
        }
        
        return completed_tensor, complete_features, reconstruction_targets


class ModalityBranch(nn.Module):
    """
    Individual modality processing branch with U-Net encoder structure.
    Extracts multi-scale features for single modality.
    """
    
    def __init__(self, base_features=32):
        super(ModalityBranch, self).__init__()
        
        self.enc1 = self._double_conv(1, base_features)
        self.enc2 = self._double_conv(base_features, base_features * 2)
        self.enc3 = self._double_conv(base_features * 2, base_features * 3) 
        self.enc4 = self._double_conv(base_features * 3, base_features * 4) 
        self.bottleneck = self._double_conv(base_features * 4, base_features * 6) 
        
        self.pool = nn.MaxPool2d(2)
        
    def _double_conv(self, in_channels, out_channels):
        """Double convolution block with batch norm and dropout."""
        return nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, padding=1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Dropout2d(0.3), 
            nn.Conv2d(out_channels, out_channels, 3, padding=1, bias=False),
            nn.BatchNorm2d(out_channels), 
            nn.ReLU(inplace=True),
            nn.Dropout2d(0.2) 
        )
    
    def forward(self, x):
        """Extract multi-scale encoder features."""
        enc1 = self.enc1(x)
        enc2 = self.enc2(self.pool(enc1))
        enc3 = self.enc3(self.pool(enc2))
        enc4 = self.enc4(self.pool(enc3))
        bottleneck = self.bottleneck(self.pool(enc4))
        
        return {
            'enc1': enc1,
            'enc2': enc2, 
            'enc3': enc3,
            'enc4': enc4,
            'bottleneck': bottleneck
        }


class PseudoFeatureGenerator(nn.Module):
    """
    Generates pseudo features for missing modality branches.
    Uses residual refinement to ensure feature consistency.
    """
    
    def __init__(self, feature_channels):
        super(PseudoFeatureGenerator, self).__init__()
        
        self.generator = nn.Sequential(
            nn.Conv2d(feature_channels, feature_channels, 3, padding=1),
            nn.BatchNorm2d(feature_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(feature_channels, feature_channels, 3, padding=1),
            nn.BatchNorm2d(feature_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(feature_channels, feature_channels, 1)
        )
        
    def forward(self, consensus_feature):
        """Generate pseudo features from consensus."""
        return self.generator(consensus_feature)


# ==================== Swin Transformer Components ====================

class WindowAttention(nn.Module):
    """
    Window-based multi-head self attention with relative position bias.
    Core component of Swin Transformer blocks.
    """
    
    def __init__(self, dim, window_size, num_heads, qkv_bias=True, attn_drop=0., proj_drop=0.):
        super().__init__()
        self.dim = dim
        self.window_size = window_size
        self.num_heads = num_heads
        head_dim = dim // num_heads
        self.scale = head_dim ** -0.5

        self.relative_position_bias_table = nn.Parameter(
            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))

        coords_h = torch.arange(self.window_size[0])
        coords_w = torch.arange(self.window_size[1])
        coords = torch.stack(torch.meshgrid([coords_h, coords_w], indexing='ij'))
        coords_flatten = torch.flatten(coords, 1)
        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]
        relative_coords = relative_coords.permute(1, 2, 0).contiguous()
        relative_coords[:, :, 0] += self.window_size[0] - 1 
        relative_coords[:, :, 1] += self.window_size[1] - 1
        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1
        relative_position_index = relative_coords.sum(-1)
        self.register_buffer("relative_position_index", relative_position_index)

        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)

        nn.init.normal_(self.relative_position_bias_table, std=.02)

    def forward(self, x, mask=None):
        """Apply window attention with relative position bias."""
        B_, N, C = x.shape
        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2]

        q = q * self.scale
        attn = (q @ k.transpose(-2, -1))

        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(
            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)
        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()
        attn = attn + relative_position_bias.unsqueeze(0)

        if mask is not None:
            nW = mask.shape[0]
            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)
            attn = attn.view(-1, self.num_heads, N, N)
            attn = F.softmax(attn, dim=-1)
        else:
            attn = F.softmax(attn, dim=-1)

        attn = self.attn_drop(attn)

        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)
        x = self.proj(x)
        x = self.proj_drop(x)
        return x


def window_partition(x, window_size):
    """Partition feature maps into non-overlapping windows."""
    B, H, W, C = x.shape
    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)
    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)
    return windows


def window_reverse(windows, window_size, H, W):
    """Reverse window partition to reconstruct feature maps."""
    B = int(windows.shape[0] / (H * W / window_size / window_size))
    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)
    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)
    return x


class SwinTransformerBlock(nn.Module):
    """
    Swin Transformer block with window/shifted-window multi-head attention.
    Supports cyclic shift for shifted window attention.
    """
    
    def __init__(self, dim, input_resolution, num_heads, window_size=4, shift_size=0,
                 mlp_ratio=4., qkv_bias=True, drop=0., attn_drop=0., drop_path=0.):
        super().__init__()
        self.dim = dim
        self.input_resolution = input_resolution
        self.num_heads = num_heads
        self.window_size = window_size
        self.shift_size = shift_size
        self.mlp_ratio = mlp_ratio
        
        if min(self.input_resolution) <= self.window_size:
            self.shift_size = 0
            self.window_size = min(self.input_resolution)
        
        self.window_size = max(1, self.window_size)
        self.shift_size = min(self.shift_size, self.window_size // 2)
        
        self.norm1 = nn.LayerNorm(dim)
        self.attn = WindowAttention(
            dim, window_size=(self.window_size, self.window_size),
            num_heads=num_heads, qkv_bias=qkv_bias, 
            attn_drop=attn_drop, proj_drop=drop)
        
        self.drop_path = nn.Identity() if drop_path == 0. else nn.Dropout(drop_path)
        self.norm2 = nn.LayerNorm(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = nn.Sequential(
            nn.Linear(dim, mlp_hidden_dim),
            nn.GELU(),
            nn.Dropout(drop),
            nn.Linear(mlp_hidden_dim, dim),
            nn.Dropout(drop)
        )

    def forward(self, x):
        """Apply attention and MLP with residual connections."""
        H, W = self.input_resolution
        B, L, C = x.shape
        
        assert L == H * W, f"input_resolution = {self.input_resolution}"
        
        shortcut = x
        x = self.norm1(x)
        x = x.view(B, H, W, C)
        
        if self.shift_size > 0:
            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))
        else:
            shifted_x = x
        
        x_windows = window_partition(shifted_x, self.window_size)
        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)
        
        attn_windows = self.attn(x_windows)
        
        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)
        shifted_x = window_reverse(attn_windows, self.window_size, H, W)
        
        if self.shift_size > 0:
            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))
        else:
            x = shifted_x
        
        x = x.view(B, H * W, C)

        x = shortcut + self.drop_path(x)
        x = x + self.drop_path(self.mlp(self.norm2(x)))
        return x


class PatchMerging(nn.Module):
    """Merge 2x2 patches for downsampling and dimension expansion."""
    
    def __init__(self, input_resolution, dim):
        super().__init__()
        self.input_resolution = input_resolution
        self.dim = dim
        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)
        self.norm = nn.LayerNorm(4 * dim)

    def forward(self, x):
        """Merge 2x2 patches and reduce dimensions."""
        H, W = self.input_resolution
        B, L, C = x.shape
        
        x = x.view(B, H, W, C)

        x0 = x[:, 0::2, 0::2, :]
        x1 = x[:, 1::2, 0::2, :]
        x2 = x[:, 0::2, 1::2, :]
        x3 = x[:, 1::2, 1::2, :]
        x = torch.cat([x0, x1, x2, x3], -1)
        x = x.view(B, -1, 4 * C)

        x = self.norm(x)
        x = self.reduction(x)

        return x


class LightweightSwinTransformerV2(nn.Module):
    """
    Lightweight Swin Transformer for classification.
    Processes 2D feature maps from encoder bottleneck.
    """
    
    def __init__(self, input_channels=3, embed_dim=96, input_resolution=8,
                 depths=[2, 2], num_heads=[3, 6], window_size=4, mlp_ratio=4., 
                 drop_rate=0.1, attn_drop_rate=0.1, drop_path_rate=0.2, num_classes=1):
        super().__init__()
        self.embed_dim = embed_dim
        self.num_layers = len(depths)
        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))
        self.input_resolution = input_resolution

        self.patch_embed = nn.Conv2d(input_channels, embed_dim, kernel_size=1)
        self.pos_drop = nn.Dropout(p=drop_rate)

        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]
        
        self.layers = nn.ModuleList()
        for i_layer in range(self.num_layers):
            current_resolution = input_resolution // (2 ** i_layer)
            current_resolution = max(1, current_resolution)
            layer_resolution = (current_resolution, current_resolution)
            layer_dim = int(embed_dim * 2 ** i_layer)
            
            adaptive_window_size = min(window_size, current_resolution // 2) if current_resolution > 2 else 1
            adaptive_window_size = max(1, adaptive_window_size)
            
            print(f"Layer {i_layer}: resolution={layer_resolution}, dim={layer_dim}, window_size={adaptive_window_size}")
            
            layer_blocks = nn.ModuleList([
                SwinTransformerBlock(
                    dim=layer_dim,
                    input_resolution=layer_resolution,
                    num_heads=num_heads[min(i_layer, len(num_heads)-1)], 
                    window_size=adaptive_window_size,
                    shift_size=0 if (j % 2 == 0) else adaptive_window_size // 2,
                    mlp_ratio=mlp_ratio,
                    qkv_bias=True,
                    drop=drop_rate,
                    attn_drop=attn_drop_rate,
                    drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])][j] if j < len(dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])]) else 0.0
                ) for j in range(depths[i_layer])
            ])
            
            if i_layer < self.num_layers - 1 and current_resolution > 1:
                downsample = PatchMerging(input_resolution=layer_resolution, dim=layer_dim)
            else:
                downsample = None
                
            self.layers.append(nn.ModuleDict({
                'blocks': layer_blocks,
                'downsample': downsample
            }))
        
        self.norm = nn.LayerNorm(self.num_features)
        self.avgpool = nn.AdaptiveAvgPool1d(1)
        
        self.head = nn.Sequential(
            nn.Linear(self.num_features, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            nn.Linear(128, 64),
            nn.BatchNorm1d(64),
            nn.ReLU(inplace=True),
            nn.Dropout(0.2),
            nn.Linear(64, num_classes)
        )

    def forward(self, x):
        """
        Process 2D feature maps through Swin layers.
        Args:
            x: [B, C, H, W] bottleneck features
        Returns:
            [B, num_classes] classification logits
        """
        B, C, H, W = x.shape
        
        expected_size = self.input_resolution
        if H != expected_size or W != expected_size:
            x = F.adaptive_avg_pool2d(x, (expected_size, expected_size))
            H, W = expected_size, expected_size

        x = self.patch_embed(x)
        B, embed_dim, H, W = x.shape
        
        x = x.flatten(2).transpose(1, 2)
        x = self.pos_drop(x)
        
        current_h, current_w = H, W
        
        for layer in self.layers:
            for block in layer['blocks']:
                block.input_resolution = (current_h, current_w)
                x = block(x)
                
            if layer['downsample'] is not None:
                layer['downsample'].input_resolution = (current_h, current_w)
                x = layer['downsample'](x)
                current_h, current_w = current_h // 2, current_w // 2
                current_h, current_w = max(1, current_h), max(1, current_w)
        
        x = self.norm(x)
        x = self.avgpool(x.transpose(1, 2))
        x = torch.flatten(x, 1)
        x = self.head(x)
        return x


# ==================== Feature Fusion Components ====================

class IntelligentFusionHub(nn.Module):
    """
    Intelligent feature fusion across modality branches.
    Handles missing modalities through consensus generation and pseudo-feature synthesis.
    """
    
    def __init__(self, base_features, img_size=(256, 256)):
        super(IntelligentFusionHub, self).__init__()
        self.base_features = base_features
        self.img_size = img_size
        
        self.generators = nn.ModuleDict({
            'enc1': PseudoFeatureGenerator(base_features),
            'enc2': PseudoFeatureGenerator(base_features * 2),
            'enc3': PseudoFeatureGenerator(base_features * 3),
            'enc4': PseudoFeatureGenerator(base_features * 4),
            'bottleneck': PseudoFeatureGenerator(base_features * 6)
        })
        
        self.fusion_weights = nn.ModuleDict({
            'enc1': nn.Conv2d(base_features * NUM_MODALITIES, base_features, 1),
            'enc2': nn.Conv2d(base_features * 2 * NUM_MODALITIES, base_features * 2, 1),
            'enc3': nn.Conv2d(base_features * 3 * NUM_MODALITIES, base_features * 3, 1),
            'enc4': nn.Conv2d(base_features * 4 * NUM_MODALITIES, base_features * 4, 1),
            'bottleneck': nn.Conv2d(base_features * 6 * NUM_MODALITIES, base_features * 6, 1)
        })

    def forward(self, modality_features, modality_mask):
        """
        Fuse features from multiple modality branches.
        Generate features for missing modalities.
        
        Args:
            modality_features: dict of per-modality encoder outputs
            modality_mask: [B, num_modalities] availability indicator
        Returns:
            fused_features: dict of fused multi-scale features
            reconstruction_targets: dict for auxiliary loss
        """
        fused_features = {}
        reconstruction_targets = {}
        
        for level in ['enc1', 'enc2', 'enc3', 'enc4', 'bottleneck']:
            available_features = []
            level_features = []
            
            for i, modality in enumerate(MODALITIES):
                if modality in modality_features:
                    feature = modality_features[modality][level]
                    available_features.append(feature)
                    level_features.append(feature)
                else:
                    level_features.append(None)
            
            # Handle case with no available features
            if len(available_features) == 0:
                B = modality_mask.shape[0]
                level_multipliers = {'enc1': 1, 'enc2': 2, 'enc3': 3, 'enc4': 4, 'bottleneck': 6}
                channels = self.base_features * level_multipliers[level]
                
                level_downsamples = {'enc1': 1, 'enc2': 2, 'enc3': 4, 'enc4': 8, 'bottleneck': 16}
                H = self.img_size[0] // level_downsamples[level]
                W = self.img_size[1] // level_downsamples[level]
                
                fused_features[level] = torch.zeros(B, channels, H, W, device=modality_mask.device)
                continue
            
            # Generate consensus feature from available modalities
            consensus_feature = torch.stack(available_features).mean(dim=0)
            
            # Complete features for all modalities
            final_features = []
            for i, modality in enumerate(MODALITIES):
                if modality_mask[:, i].sum() > 0:
                    if level_features[i] is not None:
                        final_features.append(level_features[i])
                    else:
                        # Generate pseudo feature for missing modality
                        pseudo_feature = self.generators[level](consensus_feature)
                        final_features.append(pseudo_feature)
                        if level not in reconstruction_targets:
                            reconstruction_targets[level] = {}
                        reconstruction_targets[level][modality] = pseudo_feature
                else:
                    final_features.append(torch.zeros_like(consensus_feature))
            
            # Fuse all features
            concatenated = torch.cat(final_features, dim=1)
            fused_features[level] = self.fusion_weights[level](concatenated)
        
        return fused_features, reconstruction_targets


def load_clinical_labels(clinical_file):
    """
    Load PCR classification labels from CSV file.
    
    Args:
        clinical_file: Path to clinical data CSV
    Returns:
        dict mapping patient ID to PCR label
    """
    df = pd.read_csv(clinical_file, encoding='GBK')
    df['ID'] = df['ID'].astype(str)
    id_to_pcr = dict(zip(df['ID'], df['PCR']))
    pcr_counts = df['PCR'].value_counts().to_dict()
    return id_to_pcr


# ==================== Shared Encoder Components ====================

class SharedMainEncoder(nn.Module):
    """
    Shared U-Net encoder for segmentation and classification tasks.
    Extracts multi-scale features from completed multi-modal input.
    """
    
    def __init__(self, input_channels, base_features=32):
        super(SharedMainEncoder, self).__init__()
        
        self.enc1 = self._double_conv(input_channels, base_features)
        self.enc2 = self._double_conv(base_features, base_features * 2)
        self.enc3 = self._double_conv(base_features * 2, base_features * 3)
        self.enc4 = self._double_conv(base_features * 3, base_features * 4)
        self.bottleneck = self._double_conv(base_features * 4, base_features * 6)
        
        self.pool = nn.MaxPool2d(2)
    
    def _double_conv(self, in_channels, out_channels):
        """Double convolution with batch norm and dropout."""
        return nn.Sequential(
            nn.Conv2d(in_channels, out_channels, 3, padding=1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Dropout2d(0.2),
            nn.Conv2d(out_channels, out_channels, 3, padding=1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Dropout2d(0.1)
        )
    
    def forward(self, x):
        """
        Encode features at multiple scales.
        Args:
            x: [B, 3*base_features, H, W] completed multi-modal features
        Returns:
            dict of multi-scale encoder features
        """
        enc1 = self.enc1(x)
        enc2 = self.enc2(self.pool(enc1))
        enc3 = self.enc3(self.pool(enc2))
        enc4 = self.enc4(self.pool(enc3))
        bottleneck = self.bottleneck(self.pool(enc4))
        
        return {
            'enc1': enc1,
            'enc2': enc2,
            'enc3': enc3,
            'enc4': enc4,
            'bottleneck': bottleneck
        }


class SegmentationDecoder(nn.Module):
    """
    U-Net decoder for segmentation task.
    Reconstructs full resolution segmentation mask from encoder features.
    """
    
    def __init__(self, base_features, out_channels=1):
        super(SegmentationDecoder, self).__init__()

        self.up1 = self._up_block(base_features * 6, base_features * 4)
        self.up2 = self._up_block(base_features * 4, base_features * 3)
        self.up3 = self._up_block(base_features * 3, base_features * 2)
        self.up4 = self._up_block(base_features * 2, base_features)
        
        self.output_conv = nn.Conv2d(base_features, out_channels, 1)
    
    def _up_block(self, in_channels, out_channels):
        """Upsample block with skip connection concatenation."""
        return nn.Sequential(
            nn.ConvTranspose2d(in_channels, out_channels, 2, stride=2),
            nn.Conv2d(out_channels * 2, out_channels, 3, padding=1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, 3, padding=1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU(inplace=True)
        )
    
    def forward(self, encoder_features):
        """
        Decode encoder features to full resolution.
        Args:
            encoder_features: dict from SharedMainEncoder
        Returns:
            [B, out_channels, H, W] segmentation mask
        """
        x = encoder_features['bottleneck']
        
        x = self.up1[0](x)
        x = torch.cat([x, encoder_features['enc4']], dim=1)
        x = self.up1[1:](x)
        
        x = self.up2[0](x)
        x = torch.cat([x, encoder_features['enc3']], dim=1)
        x = self.up2[1:](x)
        
        x = self.up3[0](x)
        x = torch.cat([x, encoder_features['enc2']], dim=1)
        x = self.up3[1:](x)
        
        x = self.up4[0](x)
        x = torch.cat([x, encoder_features['enc1']], dim=1)
        x = self.up4[1:](x)
        
        seg_output = self.output_conv(x)
        return seg_output

# ==================== Segmentation Feature Compression ====================

class SegmentationFeatureCompressor(nn.Module):
    """
    Compresses multi-scale encoder features from 5 adjacent slices.
    Aggregates features into a unified compressed representation for classification.
    
    Architecture:
    - 25 feature projectors: 5 slices × 5 encoder levels
    - Projects all features to 16 channels
    - Global average pooling
    - Final compression to 64 channels
    """
    
    def __init__(self, base_features, img_size=(256, 256)):
        super(SegmentationFeatureCompressor, self).__init__()
        self.img_size = img_size
        
        # Feature projectors: 5 slices × 5 encoder levels
        self.feature_projectors = nn.ModuleList([
            # Slice 1
            nn.Conv2d(base_features, 16, 1),
            nn.Conv2d(base_features * 2, 16, 1),
            nn.Conv2d(base_features * 3, 16, 1),
            nn.Conv2d(base_features * 4, 16, 1),
            nn.Conv2d(base_features * 6, 16, 1),
            # Slice 2
            nn.Conv2d(base_features, 16, 1),
            nn.Conv2d(base_features * 2, 16, 1),
            nn.Conv2d(base_features * 3, 16, 1),
            nn.Conv2d(base_features * 4, 16, 1),
            nn.Conv2d(base_features * 6, 16, 1),
            # Slice 3 (center)
            nn.Conv2d(base_features, 16, 1),
            nn.Conv2d(base_features * 2, 16, 1),
            nn.Conv2d(base_features * 3, 16, 1),
            nn.Conv2d(base_features * 4, 16, 1),
            nn.Conv2d(base_features * 6, 16, 1),
            # Slice 4
            nn.Conv2d(base_features, 16, 1),
            nn.Conv2d(base_features * 2, 16, 1),
            nn.Conv2d(base_features * 3, 16, 1),
            nn.Conv2d(base_features * 4, 16, 1),
            nn.Conv2d(base_features * 6, 16, 1),
            # Slice 5
            nn.Conv2d(base_features, 16, 1),
            nn.Conv2d(base_features * 2, 16, 1),
            nn.Conv2d(base_features * 3, 16, 1),
            nn.Conv2d(base_features * 4, 16, 1),
            nn.Conv2d(base_features * 6, 16, 1),
        ])
        
        # Adaptive pooling based on image size
        min_size = min(img_size)
        if min_size >= 256:
            self.pool_size = (8, 8)
        elif min_size >= 128:
            self.pool_size = (4, 4)
        else:
            self.pool_size = (2, 2)
        
        self.global_pool = nn.AdaptiveAvgPool2d(self.pool_size)
        
        # Final compression: 25 features × 16 channels → 128 → 64 channels
        self.final_compressor = nn.Sequential(
            nn.Conv2d(16 * 25, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True)
        )

    def forward(self, multi_slice_seg_features):
        """
        Compress multi-slice segmentation encoder features.
        
        Args:
            multi_slice_seg_features: List of feature dicts
                - 5-slice mode: list of 5 dicts
                - Single-slice mode: list of 1 dict (replicated to 5)
                - Each dict contains: {'enc1', 'enc2', 'enc3', 'enc4', 'bottleneck'}
        
        Returns:
            compressed_features: [B, 64, pool_h, pool_w] compressed feature representation
        """
        compressed_feats = []
        
        # Handle single-slice mode by replicating to 5 slices
        if len(multi_slice_seg_features) == 1:
            single_features = multi_slice_seg_features[0]
            multi_slice_seg_features = [single_features] * 5
        elif len(multi_slice_seg_features) != 5:
            raise ValueError(f"Expected 5 or 1 slices, got {len(multi_slice_seg_features)}")
        
        # Extract and compress features from each slice and encoder level
        layer_names = ['enc1', 'enc2', 'enc3', 'enc4', 'bottleneck']
        projector_idx = 0
        
        for slice_idx in range(5):
            seg_features = multi_slice_seg_features[slice_idx]
            for layer_name in layer_names:
                feat = seg_features[layer_name]
                compressed = self.feature_projectors[projector_idx](feat)
                pooled = self.global_pool(compressed)
                compressed_feats.append(pooled)
                projector_idx += 1
        
        # Stack all compressed features: 5 slices × 5 levels = 25 features
        stacked_features = torch.cat(compressed_feats, dim=1)
        
        # Final compression
        final_features = self.final_compressor(stacked_features)
        
        return final_features


# ==================== Multi-Modal 2D UNet (Unified Segmentation + Classification) ====================

class MultiModal2DUNet(nn.Module):
    """
    Unified multi-modal segmentation and classification system.
    
    Architecture overview:
    1. Modal Completion Layer: Handle missing modalities
    2. Shared Main Encoder: Extract multi-scale features
    3. Segmentation Decoder: Full-resolution segmentation mask
    4. Classification Dual-Path:
       - Primary: Swin Transformer on encoder bottleneck
       - Secondary: Segmentation feature compressor on multi-scale features
       - Fusion: Concatenate and classify
    
    Supports both single-slice and multi-slice (5-slice) processing modes.
    """
    
    def __init__(self, modalities, out_channels=1, base_features=32, 
                 img_size=(256, 256), enable_classification=True):
        super(MultiModal2DUNet, self).__init__()
        self.modalities = modalities
        self.base_features = base_features
        self.enable_classification = enable_classification
        self.img_size = img_size

        # Module 1: Modal completion layer
        self.modal_completion = ModalCompletionLayer(base_features, modalities)
        
        # Module 2: Shared main encoder
        self.main_encoder = SharedMainEncoder(
            input_channels=len(modalities) * base_features,
            base_features=base_features
        )
        
        # Module 3: Segmentation decoder
        self.segmentation_decoder = SegmentationDecoder(base_features, out_channels)
        
        # Module 4: Parallel classification branch
        if self.enable_classification:
            bottleneck_channels = base_features * 6
            bottleneck_resolution = min(img_size) // 16
            bottleneck_resolution = max(4, bottleneck_resolution)
            
            adaptive_window_size = min(4, bottleneck_resolution // 2)
            adaptive_window_size = max(2, adaptive_window_size)
            
            print(f"Classification configuration: bottleneck_channels={bottleneck_channels}, "
                  f"resolution={bottleneck_resolution}, window_size={adaptive_window_size}")
            
            # Primary path: Swin Transformer on bottleneck features
            self.swin_transformer = LightweightSwinTransformerV2(
                input_channels=bottleneck_channels,
                embed_dim=96,
                input_resolution=bottleneck_resolution,
                depths=[2, 2],
                num_heads=[3, 6],
                window_size=adaptive_window_size,
                mlp_ratio=4.0,
                drop_rate=0.1,
                attn_drop_rate=0.1,
                drop_path_rate=0.2,
                num_classes=512
            )
            
            # Secondary path: Segmentation feature compressor
            self.seg_feature_compressor = SegmentationFeatureCompressor(base_features, img_size)
            
            # Feature fusion and classification head
            pool_size = self.seg_feature_compressor.pool_size
            seg_feature_dim = 64 * pool_size[0] * pool_size[1]
            
            self.feature_fusion = nn.Sequential(
                nn.Linear(512 + seg_feature_dim, 256),
                nn.ReLU(inplace=True),
                nn.Dropout(0.3),
                nn.Linear(256, 128),
                nn.ReLU(inplace=True),
                nn.Dropout(0.2),
                nn.Linear(128, 1)
            )

    def forward(self, images, modality_mask, return_features=False, 
                return_classification=False, multi_slice_mode=False):
        """
        Forward pass for segmentation and classification.
        
        Args:
            images: Input images
                - Single-slice: [B, num_modalities, H, W]
                - Multi-slice: [B, 5, num_modalities, H, W]
            modality_mask: [B, num_modalities] availability indicator
            return_features: Return reconstruction targets
            return_classification: Return classification output
            multi_slice_mode: Process 5 adjacent slices
        
        Returns:
            dict with keys:
                - 'segmentation': [B, 1, H, W] segmentation mask
                - 'classification': [B, 1] classification logits (if requested)
                - 'reconstruction_targets': Feature targets (if requested)
                - 'complete_features': Generated features (if requested)
        """
        if multi_slice_mode:
            B, num_slices, num_modalities, H, W = images.shape
            return self._forward_multi_slice(images, modality_mask, return_features, return_classification)
        else:
            return self._forward_single_slice(images, modality_mask, return_features, return_classification)
    
    def _forward_single_slice(self, images, modality_mask, return_features=False, return_classification=False):
        """Single-slice forward pass."""
        B = images.shape[0]
        
        # Step 1: Modal completion
        completed_tensor, complete_features, reconstruction_targets = \
            self.modal_completion(images, modality_mask)
        
        # Step 2: Shared encoder
        encoder_features = self.main_encoder(completed_tensor)
        
        # Step 3: Segmentation decoder
        seg_output = self.segmentation_decoder(encoder_features)
        
        # Step 4: Classification dual-path (if enabled)
        cls_output = None
        if return_classification and self.enable_classification:
            bottleneck_features = encoder_features['bottleneck']
            
            # Primary path: Swin Transformer
            swin_features = self.swin_transformer(bottleneck_features)
            
            # Secondary path: Segmentation feature compression
            seg_compressed_features = self.seg_feature_compressor([encoder_features])
            seg_features_flat = seg_compressed_features.view(B, -1)
            
            # Feature fusion
            combined_features = torch.cat([swin_features, seg_features_flat], dim=1)
            cls_output = self.feature_fusion(combined_features)
        
        result = {'segmentation': seg_output}
        if return_classification:
            result['classification'] = cls_output
        if return_features:
            result['reconstruction_targets'] = reconstruction_targets
            result['complete_features'] = complete_features
        
        return result
    
    def _forward_multi_slice(self, images, modality_mask, return_features=False, return_classification=False):
        """
        Multi-slice forward pass (5 adjacent slices).
        Performs segmentation only on center slice.
        Uses all 5 slices for classification feature extraction.
        
        Args:
            images: [B, 5, num_modalities, H, W]
            modality_mask: [B, num_modalities]
        """
        B, num_slices, num_modalities, H, W = images.shape
        center_idx = 2
        
        # Step 1: Process center slice for segmentation
        center_images = images[:, center_idx]
        
        completed_tensor, complete_features, reconstruction_targets = \
            self.modal_completion(center_images, modality_mask)
        
        center_encoder_features = self.main_encoder(completed_tensor)
        seg_output = self.segmentation_decoder(center_encoder_features)
        
        # Step 2: Extract features from all 5 slices for classification
        cls_output = None
        if return_classification and self.enable_classification:
            multi_slice_seg_features = []
            
            for slice_idx in range(num_slices):
                slice_images = images[:, slice_idx]
                completed_tensor_slice, _, _ = self.modal_completion(slice_images, modality_mask)
                encoder_features = self.main_encoder(completed_tensor_slice)
                multi_slice_seg_features.append(encoder_features)
            
            # Primary path: Swin on center bottleneck
            center_bottleneck = multi_slice_seg_features[center_idx]['bottleneck']
            swin_features = self.swin_transformer(center_bottleneck)
            
            # Secondary path: Compress features from all 5 slices
            seg_compressed_features = self.seg_feature_compressor(multi_slice_seg_features)
            seg_features_flat = seg_compressed_features.view(B, -1)
            
            # Feature fusion
            combined_features = torch.cat([swin_features, seg_features_flat], dim=1)
            cls_output = self.feature_fusion(combined_features)
        
        result = {'segmentation': seg_output}
        if return_classification:
            result['classification'] = cls_output
        if return_features:
            result['reconstruction_targets'] = reconstruction_targets
            result['complete_features'] = complete_features
        
        return result


# ==================== Medical Image Augmentation ====================

class MedicalImageAugmentation:
    """
    Comprehensive data augmentation for medical images.
    
    Supports geometric and intensity transformations optimized for reducing overfitting.
    
    Transforms:
    - Geometric: rotation, scaling, translation, shearing, elastic deformation, perspective
    - Intensity: gamma correction, brightness, contrast, noise, blur
    - Spatial: horizontal/vertical flipping
    """
    
    def __init__(self, 
                 rotation_range=15,
                 scale_range=(0.9, 1.1),
                 translation_range=0.1,
                 shear_range=5,
                 elastic_alpha=500,
                 elastic_sigma=20,
                 gamma_range=(0.8, 1.2),
                 brightness_add_range=(-0.1, 0.1),
                 contrast_range=(0.8, 1.2),
                 noise_std_range=(0, 0.02),
                 blur_kernel_size=3,
                 prob_geometric=0.7,
                 prob_elastic=0.4,
                 prob_perspective=0.3,
                 prob_flip_h=0.5,
                 prob_flip_v=0.3,
                 prob_gamma=0.5,
                 prob_brightness=0.0,
                 prob_contrast=0.5,
                 prob_noise=0.3,
                 prob_blur=0.2):

        self.rotation_range = rotation_range
        self.scale_range = scale_range
        self.translation_range = translation_range
        self.shear_range = shear_range
        self.elastic_alpha = elastic_alpha
        self.elastic_sigma = elastic_sigma
        self.gamma_range = gamma_range
        self.brightness_add_range = brightness_add_range
        self.contrast_range = contrast_range
        self.noise_std_range = noise_std_range
        self.blur_kernel_size = blur_kernel_size
        
        self.prob_geometric = prob_geometric
        self.prob_elastic = prob_elastic
        self.prob_perspective = prob_perspective
        self.prob_flip_h = prob_flip_h
        self.prob_flip_v = prob_flip_v
        self.prob_gamma = prob_gamma
        self.prob_brightness = prob_brightness
        self.prob_contrast = prob_contrast
        self.prob_noise = prob_noise
        self.prob_blur = prob_blur

    def elastic_transform(self, images, alpha, sigma):
        """
        Apply elastic deformation using Gaussian random displacement fields.
        
        Args:
            images: List of image arrays
            alpha: Deformation strength
            sigma: Gaussian kernel standard deviation
        Returns:
            List of elastically deformed images
        """
        if not isinstance(images, list):
            images = [images]
        
        shape = images[0].shape[:2]
        h, w = shape
        
        # Generate random displacement fields
        dx = gaussian_filter((np.random.rand(*shape) * 2 - 1), sigma, mode="constant", cval=0) * alpha
        dy = gaussian_filter((np.random.rand(*shape) * 2 - 1), sigma, mode="constant", cval=0) * alpha
        
        # Create coordinate grids
        x, y = np.meshgrid(np.arange(w), np.arange(h))
        map_x = (x + dx).astype(np.float32)
        map_y = (y + dy).astype(np.float32)
        
        transformed_images = []
        for img in images:
            transformed = cv2.remap(img, map_x, map_y, cv2.INTER_LINEAR, borderMode=cv2.BORDER_REFLECT)
            transformed_images.append(transformed)
            
        return transformed_images

    def geometric_transform(self, images, angle=0, scale=1.0, translate=(0, 0), shear=0):
        """
        Apply combined geometric transformations (rotation, scaling, translation, shearing).
        
        Args:
            images: List of image arrays
            angle: Rotation angle in degrees
            scale: Scaling factor
            translate: (tx, ty) translation
            shear: Shear angle in degrees
        Returns:
            List of geometrically transformed images
        """
        if not isinstance(images, list):
            images = [images]
            
        if len(images) == 0:
            return images
            
        h, w = images[0].shape[:2]
        center = (w / 2, h / 2)
        
        # Build combined transformation matrix
        M_rot = cv2.getRotationMatrix2D(center, angle, scale)
        M_rot[0, 2] += translate[0]
        M_rot[1, 2] += translate[1]
        
        # Add shearing
        if shear != 0:
            shear_rad = np.deg2rad(shear)
            M_shear = np.array([[1, np.tan(shear_rad), 0], [0, 1, 0]], dtype=np.float32)
            M_combined = np.dot(M_shear, np.vstack([M_rot, [0, 0, 1]]))[:2]
        else:
            M_combined = M_rot
            
        transformed_images = []
        for img in images:
            transformed = cv2.warpAffine(img, M_combined, (w, h), 
                                       flags=cv2.INTER_LINEAR, 
                                       borderMode=cv2.BORDER_REFLECT)
            transformed_images.append(transformed)
            
        return transformed_images

    def perspective_transform(self, images, distortion_scale=0.05):
        """
        Apply random perspective transformation.
        
        Args:
            images: List of image arrays
            distortion_scale: Maximum corner displacement as fraction of image size
        Returns:
            List of perspective-transformed images
        """
        if not isinstance(images, list):
            images = [images]
            
        if len(images) == 0:
            return images
            
        h, w = images[0].shape[:2]
        offset = distortion_scale * min(h, w)
        
        # Original corners
        src_points = np.array([[0, 0], [w-1, 0], [w-1, h-1], [0, h-1]], dtype=np.float32)
        
        # Random perturbation
        random_offset = np.random.uniform(-offset, offset, (4, 2))
        dst_points = src_points + random_offset
        
        # Clip to image bounds
        dst_points[:, 0] = np.clip(dst_points[:, 0], offset, w-1-offset)
        dst_points[:, 1] = np.clip(dst_points[:, 1], offset, h-1-offset)
        dst_points = dst_points.astype(np.float32)
        
        try:
            M = cv2.getPerspectiveTransform(src_points, dst_points)
            transformed_images = []
            for img in images:
                transformed = cv2.warpPerspective(img, M, (w, h), 
                                                flags=cv2.INTER_LINEAR, 
                                                borderMode=cv2.BORDER_REFLECT)
                transformed_images.append(transformed)
            return transformed_images
        except cv2.error:
            return images

    def adjust_gamma(self, image, gamma=1.0):
        """Apply gamma correction."""
        if gamma == 1.0:
            return image
        return np.clip(image ** gamma, 0, 1)

    def adjust_brightness(self, image, value=0.0):
        """Adjust image brightness by additive value."""
        if value == 0.0:
            return image
        return np.clip(image + value, 0, 1)

    def adjust_contrast(self, image, factor=1.0):
        """Adjust image contrast multiplicatively."""
        if factor == 1.0:
            return image
        mean = np.mean(image)
        return np.clip((image - mean) * factor + mean, 0, 1)

    def add_noise(self, image, std=0.02):
        """Add Gaussian noise to image."""
        if std == 0.0:
            return image
        noise = np.random.normal(0, std, image.shape)
        return np.clip(image + noise, 0, 1)

    def apply_blur(self, image, kernel_size=3):
        """Apply Gaussian blur."""
        if kernel_size <= 1:
            return image
        ksize = int(kernel_size) // 2 * 2 + 1
        return cv2.GaussianBlur(image, (ksize, ksize), 0)

    def apply_augmentation(self, images, mask):
        """
        Apply full augmentation pipeline to multi-modal images and segmentation mask.
        
        Compatible with dataset loader interface.
        
        Args:
            images: [num_modalities, H, W] image array
            mask: [H, W] segmentation mask
        
        Returns:
            augmented_images: [num_modalities, H, W]
            augmented_mask: [H, W]
        """
        try:
            # Convert to float and normalize
            images_list = []
            for i in range(images.shape[0]):
                img = images[i].astype(np.float32)
                if img.max() > 1.0:
                    img = img / 255.0
                images_list.append(img)
            
            # Add mask to processing list
            mask_float = mask.astype(np.float32)
            if mask_float.max() > 1.0:
                mask_float = mask_float / 255.0
            images_list.append(mask_float)
            
            # Apply spatial transformations
            if random.random() < self.prob_flip_h:
                images_list = [cv2.flip(img, 1) for img in images_list]
            if random.random() < self.prob_flip_v:
                images_list = [cv2.flip(img, 0) for img in images_list]
            
            # Geometric transformation
            if random.random() < self.prob_geometric:
                angle = random.uniform(-self.rotation_range, self.rotation_range)
                scale = random.uniform(*self.scale_range)
                h, w = images_list[0].shape[:2]
                tx = random.uniform(-self.translation_range, self.translation_range) * w
                ty = random.uniform(-self.translation_range, self.translation_range) * h
                shear = random.uniform(-self.shear_range, self.shear_range)
                
                images_list = self.geometric_transform(images_list, angle, scale, (tx, ty), shear)
            
            # Elastic deformation
            if random.random() < self.prob_elastic:
                images_list = self.elastic_transform(images_list, self.elastic_alpha, self.elastic_sigma)
            
            # Perspective transformation
            if random.random() < self.prob_perspective:
                images_list = self.perspective_transform(images_list, distortion_scale=0.03)
            
            # Intensity transformations (only on modalities, not mask)
            num_images = images.shape[0]
            for i in range(num_images):
                img = images_list[i]
                if random.random() < self.prob_gamma:
                    img = self.adjust_gamma(img, random.uniform(*self.gamma_range))
                if random.random() < self.prob_brightness:
                    img = self.adjust_brightness(img, random.uniform(*self.brightness_add_range))
                if random.random() < self.prob_contrast:
                    img = self.adjust_contrast(img, random.uniform(*self.contrast_range))
                if random.random() < self.prob_noise:
                    img = self.add_noise(img, random.uniform(*self.noise_std_range))
                if random.random() < self.prob_blur:
                    img = self.apply_blur(img, self.blur_kernel_size)
                images_list[i] = img
            
            # Post-processing
            images_list = [np.clip(img, 0, 1) for img in images_list]
            
            # Binarize mask
            mask_augmented = (images_list[-1] > 0.5).astype(np.float32)
            
            # Reconstruct output
            images_augmented = np.stack(images_list[:-1], axis=0)
            
            return images_augmented, mask_augmented
            
        except Exception as e:
            print(f"Augmentation failed: {e}")
            return images, mask

# ==================== 2D Augmentation ====================

class Augmentation2D(MedicalImageAugmentation):
    """
    2D augmentation wrapper for training and validation modes.
    
    Provides configuration presets:
    - Training mode: Aggressive augmentation for data diversity
    - Enhanced mode: Even stronger augmentation for minority PCR=1 samples
    - Validation/Test mode: No augmentation
    """
    
    def __init__(self, is_training=True, enhanced_mode=False):
        if is_training:
            if enhanced_mode:
                # Enhanced augmentation for PCR=1 minority samples
                super().__init__(
                    # Geometric transformation parameters
                    rotation_range=8,
                    scale_range=(0.9, 1.1),
                    translation_range=0.03,
                    shear_range=8,
                    elastic_alpha=300,
                    elastic_sigma=15,
                    
                    # Intensity transformation parameters
                    gamma_range=(0.9, 1.31),
                    brightness_add_range=(-0.05, 0.05),
                    contrast_range=(0.9, 1.1),
                    noise_std_range=(0, 0.03),
                    blur_kernel_size=3,
                    
                    # Transformation probabilities - more aggressive
                    prob_geometric=0.8,
                    prob_elastic=0.4,
                    prob_perspective=0.4,
                    prob_flip_h=0.5,
                    prob_flip_v=0.0,
                    
                    # Intensity transformation probabilities
                    prob_gamma=0.3,
                    prob_brightness=0.0,
                    prob_contrast=0.5,
                    prob_noise=0.3,
                    prob_blur=0.0
                )
            else:
                # Standard training augmentation
                super().__init__(
                    # Geometric transformation parameters
                    rotation_range=8,
                    scale_range=(0.95, 1.05),
                    translation_range=0.03,
                    shear_range=3,
                    elastic_alpha=200,
                    elastic_sigma=10,
                    
                    # Intensity transformation parameters
                    gamma_range=(0.9, 1.1),
                    brightness_add_range=(-0.05, 0.05),
                    contrast_range=(0.9, 1.1),
                    noise_std_range=(0, 0.05),
                    blur_kernel_size=3,
                    
                    # Transformation probabilities
                    prob_geometric=0.5,
                    prob_elastic=0.4,
                    prob_perspective=0.3,
                    prob_flip_h=0.5,
                    prob_flip_v=0.0,
                    
                    # Intensity transformation probabilities
                    prob_gamma=0.3,
                    prob_brightness=0.0,
                    prob_contrast=0.3,
                    prob_noise=0.4,
                    prob_blur=0.2
                )
        else:
            # No augmentation for validation/test
            super().__init__(
                prob_geometric=0, prob_elastic=0, prob_perspective=0,
                prob_flip_h=0, prob_flip_v=0, prob_gamma=0,
                prob_brightness=0, prob_contrast=0, prob_noise=0, prob_blur=0
            )
        
        self.is_training = is_training


# ==================== 2D Dataset ====================

class MultiModal2DDataset(Dataset):
    """
    Multi-modal 2D medical image dataset with flexible slice loading.
    
    Features:
    - Handles missing modalities (T1, T2, T1C)
    - Single-slice and multi-slice (5-adjacent) modes
    - Finds maximum lesion slice for segmentation evaluation
    - PCR classification labels support
    - Data augmentation with probability-based strategies
    
    Directory structure:
        sample_id/
        ├── T1/ori/  (T1-weighted images)
        ├── T1/mask/ (segmentation masks)
        ├── T2/ori/  (T2-weighted images)
        ├── T2/mask/
        ├── T1C/ori/ (T1 contrast-enhanced images)
        └── T1C/mask/
    """
    
    def __init__(self, data_dir, img_size, modalities, id_to_pcr=None, 
                 is_training=True, use_max_slice_only=False, use_multi_slice=False):
        self.data_dir = data_dir
        self.height, self.width = img_size
        self.modalities = modalities
        self.id_to_pcr = id_to_pcr
        self.is_training = is_training
        self.use_max_slice_only = use_max_slice_only
        self.use_multi_slice = use_multi_slice
        self.augmentation = Augmentation2D(is_training)
        
        self.samples = []
        if not os.path.exists(data_dir):
            print(f"Warning: Data directory {data_dir} does not exist")
            return
        
        # Load sample indices
        sample_ids = [d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))]
        for sample_id in sample_ids:
            sample_path = os.path.join(data_dir, sample_id)
            if any(os.path.exists(os.path.join(sample_path, mod, 'ori')) for mod in self.modalities):
                pcr_label = self.id_to_pcr.get(sample_id, 0) if self.id_to_pcr else 0
                has_pcr = sample_id in self.id_to_pcr if self.id_to_pcr else False
                
                self.samples.append({
                    'sample_id': sample_id,
                    'path': sample_path,
                    'pcr_label': pcr_label,
                    'has_pcr': has_pcr
                })
        
        # Log statistics
        if self.id_to_pcr:
            pcr_samples = sum(1 for s in self.samples if s['has_pcr'])
            pcr_counts = {}
            for sample in self.samples:
                if sample['has_pcr']:
                    pcr = sample['pcr_label']
                    pcr_counts[pcr] = pcr_counts.get(pcr, 0) + 1
            print(f"2D Dataset {os.path.basename(data_dir)}: Total={len(self.samples)}, "
                  f"With PCR labels={pcr_samples}, Distribution={pcr_counts}")
        else:
            print(f"2D Dataset {os.path.basename(data_dir)}: Total samples={len(self.samples)}")
        
        print(f"Training mode={'ON' if is_training else 'OFF'}, "
              f"Max slice only={'ON' if use_max_slice_only else 'OFF'}, "
              f"Multi-slice mode={'ON' if use_multi_slice else 'OFF'}")
    
    def __len__(self):
        if self.is_training:
            # Estimate: assuming ~50 slices per sample
            return len(self.samples) * 50
        else:
            # One sample per data item
            return len(self.samples)
    
    def __getitem__(self, idx):
        if self.is_training:
            # Random sample and slice during training
            sample_idx = idx % len(self.samples)
            sample_info = self.samples[sample_idx]
            slice_idx = None
        else:
            # Fixed sample, maximum lesion slice during validation
            sample_info = self.samples[idx]
            slice_idx = self._find_max_lesion_slice(sample_info['path'])
        
        if self.use_multi_slice and not self.is_training:
            return self._load_multi_slice_sample(sample_info, slice_idx)
        else:
            return self._load_single_slice_sample(sample_info, slice_idx)
    
    def _find_max_lesion_slice(self, sample_path):
        """
        Find slice with maximum lesion area across all modalities.
        
        Args:
            sample_path: Path to sample directory
        Returns:
            Index of slice with largest lesion
        """
        max_area = 0
        max_slice_idx = 0
        
        for modality in self.modalities:
            mask_path = os.path.join(sample_path, modality, 'mask')
            if os.path.exists(mask_path):
                mask_files = sorted([f for f in os.listdir(mask_path) if f.endswith('.png')],
                                  key=self._natural_sort_key)
                
                for i, mask_file in enumerate(mask_files):
                    mask_img = cv2.imread(os.path.join(mask_path, mask_file), cv2.IMREAD_GRAYSCALE)
                    if mask_img is not None:
                        area = (mask_img > 0).sum()
                        if area > max_area:
                            max_area = area
                            max_slice_idx = i
                break
        
        return max_slice_idx
    
    def _natural_sort_key(self, s, _nsre=re.compile('([0-9]+)')):
        """Natural sorting for file names."""
        return [int(text) if text.isdigit() else text.lower() for text in _nsre.split(s)]
    
    def _load_single_slice_sample(self, sample_info, slice_idx=None):
        """
        Load single slice with all available modalities.
        
        Args:
            sample_info: Sample metadata dict
            slice_idx: Specific slice index (None for random)
        
        Returns:
            dict with 'image', 'mask', 'modality_mask', 'pcr_label', etc.
        """
        sample_id = sample_info['sample_id']
        base_path = sample_info['path']
        
        images = []
        modality_mask = []
        mask = None
        
        for modality in self.modalities:
            mod_path = os.path.join(base_path, modality)
            ori_path = os.path.join(mod_path, 'ori')
            
            if os.path.exists(ori_path):
                image = self._load_slice(ori_path, slice_idx)
                modality_mask.append(1)
                
                if mask is None:
                    mask_path = os.path.join(mod_path, 'mask')
                    if os.path.exists(mask_path):
                        mask = self._load_slice(mask_path, slice_idx)
            else:
                image = np.zeros((self.height, self.width), dtype=np.float32)
                modality_mask.append(0)
            
            images.append(image)
        
        if mask is None:
            mask = np.zeros((self.height, self.width), dtype=np.float32)
        
        images_stack = np.stack(images, axis=0)
        
        # Apply augmentation
        if self.is_training:
            images_stack, mask = self.augmentation.apply_augmentation(images_stack, mask)
        
        # Convert to tensors
        image_tensor = torch.from_numpy(images_stack.copy()).float()
        mask_tensor = torch.from_numpy(mask.copy()).float().unsqueeze(0)
        
        return {
            'image': image_tensor,
            'mask': mask_tensor,
            'modality_mask': torch.tensor(modality_mask, dtype=torch.float32),
            'pcr_label': torch.tensor(sample_info['pcr_label'], dtype=torch.float32),
            'has_pcr': sample_info['has_pcr'],
            'sample_id': sample_id,
            'slice_idx': slice_idx if slice_idx is not None else -1
        }
    
    def _load_slice(self, folder_path, slice_idx=None):
        """
        Load image from folder, optionally selecting specific slice.
        
        Args:
            folder_path: Path to folder containing PNG images
            slice_idx: Specific slice index (None for random)
        
        Returns:
            [H, W] normalized image array
        """
        files = sorted([f for f in os.listdir(folder_path) if f.endswith('.png')],
                      key=self._natural_sort_key)
        
        if len(files) == 0:
            return np.zeros((self.height, self.width), dtype=np.float32)
        
        if slice_idx is None:
            slice_idx = random.randint(0, len(files) - 1)
        else:
            slice_idx = min(slice_idx, len(files) - 1)
        
        img_path = os.path.join(folder_path, files[slice_idx])
        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
        
        if img is not None:
            img = cv2.resize(img, (self.width, self.height))
            img = img.astype(np.float32) / 255.0
            return img
        else:
            return np.zeros((self.height, self.width), dtype=np.float32)
    
    def _load_multi_slice_sample(self, sample_info, center_slice_idx):
        """
        Load 5 adjacent slices for multi-slice classification.
        Slices: [x-2, x-1, x, x+1, x+2] where x is center slice.
        
        Args:
            sample_info: Sample metadata dict
            center_slice_idx: Center slice index
        
        Returns:
            dict with [5, 3, H, W] images and [5, H, W] masks
        """
        sample_id = sample_info['sample_id']
        base_path = sample_info['path']
        
        # Find total number of slices
        total_slices = 0
        for modality in self.modalities:
            ori_path = os.path.join(base_path, modality, 'ori')
            if os.path.exists(ori_path):
                files = [f for f in os.listdir(ori_path) if f.endswith('.png')]
                total_slices = max(total_slices, len(files))
                break
        
        if total_slices == 0:
            return self._create_empty_multi_slice_sample(sample_info)
        
        # Compute 5-slice indices with boundary clipping
        slice_indices = [
            max(0, center_slice_idx - 2),
            max(0, center_slice_idx - 1),
            center_slice_idx,
            min(total_slices - 1, center_slice_idx + 1),
            min(total_slices - 1, center_slice_idx + 2)
        ]
        
        multi_slice_images = []
        multi_slice_masks = []
        modality_mask = []
        
        # Check modality availability
        for modality in self.modalities:
            ori_path = os.path.join(base_path, modality, 'ori')
            if os.path.exists(ori_path):
                modality_mask.append(1)
            else:
                modality_mask.append(0)
        
        # Load all slices
        for slice_idx in slice_indices:
            slice_images = []
            slice_mask = None
            
            for modality in self.modalities:
                mod_path = os.path.join(base_path, modality)
                ori_path = os.path.join(mod_path, 'ori')
                
                if os.path.exists(ori_path):
                    image = self._load_slice(ori_path, slice_idx)
                    
                    if slice_mask is None:
                        mask_path = os.path.join(mod_path, 'mask')
                        if os.path.exists(mask_path):
                            slice_mask = self._load_slice(mask_path, slice_idx)
                else:
                    image = np.zeros((self.height, self.width), dtype=np.float32)
                
                slice_images.append(image)
            
            if slice_mask is None:
                slice_mask = np.zeros((self.height, self.width), dtype=np.float32)
            
            multi_slice_images.append(slice_images)
            multi_slice_masks.append(slice_mask)
        
        # Convert to numpy arrays: [5, 3, H, W]
        multi_slice_images = np.array(multi_slice_images)
        multi_slice_masks = np.array(multi_slice_masks)
        modality_mask = np.array(modality_mask, dtype=np.float32)
        
        # Augment only center slice
        center_image = multi_slice_images[2]
        center_mask = multi_slice_masks[2]
        
        if self.is_training:
            center_image, center_mask = self.augmentation.apply_augmentation(center_image, center_mask)
            multi_slice_images[2] = center_image
            multi_slice_masks[2] = center_mask
        
        # Convert to tensors
        multi_slice_images = torch.from_numpy(multi_slice_images).float()
        multi_slice_masks = torch.from_numpy(multi_slice_masks).float()
        modality_mask = torch.from_numpy(modality_mask).float()
        
        return {
            'images': multi_slice_images,
            'masks': multi_slice_masks,
            'modality_mask': modality_mask,
            'sample_id': sample_id,
            'pcr_label': sample_info['pcr_label'],
            'has_pcr': sample_info['has_pcr'],
            'slice_indices': slice_indices
        }
    
    def _create_empty_multi_slice_sample(self, sample_info):
        """Create zero-filled multi-slice sample."""
        multi_slice_images = np.zeros((5, len(self.modalities), self.height, self.width), dtype=np.float32)
        multi_slice_masks = np.zeros((5, self.height, self.width), dtype=np.float32)
        modality_mask = np.zeros(len(self.modalities), dtype=np.float32)
        
        return {
            'images': torch.from_numpy(multi_slice_images).float(),
            'masks': torch.from_numpy(multi_slice_masks).float(),
            'modality_mask': torch.from_numpy(modality_mask).float(),
            'sample_id': sample_info['sample_id'],
            'pcr_label': sample_info['pcr_label'],
            'has_pcr': sample_info['has_pcr'],
            'slice_indices': [0, 0, 0, 0, 0]
        }


# ==================== Loss Functions ====================

class DiceLoss(nn.Module):
    """Dice coefficient loss for segmentation."""
    
    def __init__(self, smooth=1e-5):
        super(DiceLoss, self).__init__()
        self.smooth = smooth
    
    def forward(self, pred, target):
        """
        Args:
            pred: [B, C, H, W] logits
            target: [B, C, H, W] binary masks
        Returns:
            Scalar loss value
        """
        pred = torch.sigmoid(pred)
        intersection = (pred * target).sum(dim=(2, 3))
        union = pred.sum(dim=(2, 3)) + target.sum(dim=(2, 3))
        dice = (2. * intersection + self.smooth) / (union + self.smooth)
        return (1 - dice).mean()


class FocalLoss(nn.Module):
    """
    Focal loss for severe class imbalance in binary classification.
    
    Enhances focus on hard-to-classify samples by weighting down easy negatives.
    
    References:
    - Lin et al. "Focal Loss for Dense Object Detection" (ICCV 2017)
    """
    
    def __init__(self, alpha=0.8, gamma=2.0, pos_weight=None, reduction='mean'):
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.pos_weight = pos_weight
        self.reduction = reduction
        
        print(f"Focal Loss: alpha={alpha}, gamma={gamma}, pos_weight={pos_weight}")
    
    def forward(self, inputs, targets):
        """
        Args:
            inputs: [B] or [B, 1] logits
            targets: [B] or [B, 1] binary labels
        Returns:
            Scalar loss value
        """
        # BCE loss with optional positive weight
        if self.pos_weight is not None:
            bce_loss = F.binary_cross_entropy_with_logits(
                inputs, targets, pos_weight=self.pos_weight, reduction='none'
            )
        else:
            bce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')
        
        # Compute probability
        pt = torch.exp(-bce_loss)
        
        # Apply alpha weight: higher for minority class
        alpha_t = self.alpha * targets + (1 - self.alpha) * (1 - targets)
        
        # Focal weight: emphasize hard samples
        focal_weight = alpha_t * (1 - pt) ** self.gamma
        
        focal_loss = focal_weight * bce_loss
        
        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        else:
            return focal_loss


class ModalReconstructionLoss(nn.Module):
    """
    Auxiliary loss for cross-modal feature reconstruction.
    Ensures generated features for missing modalities are realistic.
    """
    
    def __init__(self):
        super(ModalReconstructionLoss, self).__init__()
        self.l1_loss = nn.L1Loss()
    
    def forward(self, generated_features, target_features):
        """
        Args:
            generated_features: dict {modality: [B, C, H, W]} generated features
            target_features: dict {modality: [B, C, H, W]} ground truth features
        Returns:
            Scalar reconstruction loss
        """
        total_loss = 0
        count = 0
        
        for modality in target_features:
            if modality in generated_features:
                l1 = self.l1_loss(generated_features[modality], target_features[modality])
                total_loss += l1
                count += 1
        
        return total_loss / count if count > 0 else torch.tensor(0.0, device=next(iter(target_features.values())).device)


class CombinedLossWithReconstruction(nn.Module):
    """
    Combined segmentation and reconstruction loss.
    
    Components:
    - Dice loss: Primary segmentation metric
    - Binary cross-entropy: Complementary segmentation loss
    - Modal reconstruction: Auxiliary loss for missing modality handling
    """
    
    def __init__(self, dice_weight=1.0, ce_weight=0.3, reconstruction_weight=0.1):
        super(CombinedLossWithReconstruction, self).__init__()
        self.dice_loss = DiceLoss()
        self.ce_loss = nn.BCEWithLogitsLoss()
        self.modal_reconstruction_loss = ModalReconstructionLoss()
        
        self.dice_weight = dice_weight
        self.ce_weight = ce_weight
        self.reconstruction_weight = reconstruction_weight
        
        print(f"Combined loss: Dice weight={dice_weight}, CE weight={ce_weight}, "
              f"Reconstruction weight={reconstruction_weight}")
    
    def forward(self, pred, target, reconstruction_targets=None, complete_features=None):
        """
        Args:
            pred: [B, 1, H, W] segmentation logits
            target: [B, 1, H, W] segmentation masks
            reconstruction_targets: dict of target feature maps
            complete_features: dict of generated features
        Returns:
            Scalar total loss
        """
        # Segmentation loss
        dice_loss = self.dice_loss(pred, target)
        ce_loss = self.ce_loss(pred, target)
        seg_loss = self.dice_weight * dice_loss + self.ce_weight * ce_loss
        
        total_loss = seg_loss
        
        # Modal reconstruction loss
        if reconstruction_targets and complete_features:
            recon_loss = self.modal_reconstruction_loss(complete_features, reconstruction_targets)
            total_loss += self.reconstruction_weight * recon_loss
        
        return total_loss


# ==================== Evaluation Metrics ====================

def dice_coefficient(pred, target, smooth=1e-5):
    """
    Compute Dice similarity coefficient.
    
    Args:
        pred: [B, C, H, W] logits or predictions
        target: [B, C, H, W] binary masks
    Returns:
        Float Dice coefficient value
    """
    pred = torch.sigmoid(pred)
    pred = (pred > 0.5).float()
    intersection = (pred * target).sum()
    union = pred.sum() + target.sum()
    return ((2. * intersection + smooth) / (union + smooth)).item()


def evaluate_classification(model, dataloader, device, num_samples=50):
    """
    Evaluate classification performance on PCR prediction task.
    
    Args:
        model: Classification model
        dataloader: DataLoader providing images and PCR labels
        device: Computation device
        num_samples: Maximum samples to evaluate
    
    Returns:
        dict with metrics: accuracy, precision, recall, f1, auc, class-specific accuracies
    """
    if dataloader is None or len(dataloader.dataset) == 0:
        return {}
    
    model.eval()
    all_preds = []
    all_labels = []
    all_probs = []
    
    with torch.no_grad():
        sample_count = 0
        for batch in tqdm(dataloader, desc="Classification evaluation", leave=False):
            if sample_count >= num_samples:
                break
            
            # Handle both single and multi-slice data formats
            if 'images' in batch:
                images = batch['images'].to(device)
                modality_masks = batch['modality_mask'].to(device)
                pcr_labels = batch['pcr_label'].to(device)
                has_pcr = batch['has_pcr']
                multi_slice_mode = True
            else:
                images = batch['image'].to(device)
                modality_masks = batch['modality_mask'].to(device)
                pcr_labels = batch['pcr_label'].to(device)
                has_pcr = batch['has_pcr']
                multi_slice_mode = False
            
            # Only evaluate samples with PCR labels
            valid_indices = [i for i, has in enumerate(has_pcr) if has]
            if not valid_indices:
                continue
            
            valid_images = images[valid_indices]
            valid_modality_masks = modality_masks[valid_indices]
            valid_pcr_labels = pcr_labels[valid_indices]
            
            # Forward pass
            outputs = model(valid_images, valid_modality_masks, return_classification=True, multi_slice_mode=multi_slice_mode)
            cls_output = outputs.get('classification', None)
            
            if cls_output is not None:
                probs = torch.sigmoid(cls_output).cpu().numpy().flatten()
                preds = (probs > 0.5).astype(int)
                labels = valid_pcr_labels.cpu().numpy()
                
                all_preds.extend(preds)
                all_labels.extend(labels)
                all_probs.extend(probs)
                sample_count += len(valid_indices)
    
    model.train()
    
    if len(all_labels) == 0:
        return {}
    
    # Compute metrics
    accuracy = accuracy_score(all_labels, all_preds)
    precision = precision_score(all_labels, all_preds, zero_division=0)
    recall = recall_score(all_labels, all_preds, zero_division=0)
    f1 = f1_score(all_labels, all_preds, zero_division=0)
    
    auc = 0.0
    if len(set(all_labels)) > 1:
        auc = roc_auc_score(all_labels, all_probs)
    
    # Class-specific accuracies
    class_0_acc = 0.0
    class_1_acc = 0.0
    
    class_0_mask = np.array(all_labels) == 0
    if class_0_mask.sum() > 0:
        class_0_acc = (np.array(all_preds)[class_0_mask] == np.array(all_labels)[class_0_mask]).mean()
    
    class_1_mask = np.array(all_labels) == 1
    if class_1_mask.sum() > 0:
        class_1_acc = (np.array(all_preds)[class_1_mask] == np.array(all_labels)[class_1_mask]).mean()
    
    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'auc': auc,
        'class_0_acc': class_0_acc,
        'class_1_acc': class_1_acc,
        'num_samples': len(all_labels)
    }


def evaluate_dataset(model, dataloader, criterion, device, num_samples=50, enable_classification=False):
    """
    Evaluate dataset segmentation and optionally classification performance.
    
    Args:
        model: Model to evaluate
        dataloader: DataLoader with batches
        criterion: Loss function
        device: Computation device
        num_samples: Maximum samples to evaluate
        enable_classification: Whether to compute classification metrics
    
    Returns:
        (avg_loss, avg_dice, classification_metrics_dict)
    """
    if dataloader is None or len(dataloader.dataset) == 0:
        return 0.0, 0.0, {}
    
    model.eval()
    total_loss, total_dice, count = 0.0, 0.0, 0
    dice_scores = []
    
    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Dataset evaluation", leave=False):
            if count >= num_samples:
                break
            
            if 'images' in batch:
                # Multi-slice data
                images = batch['images'].to(device)
                masks = batch['masks'].to(device)
                modality_masks = batch['modality_mask'].to(device)
                multi_slice_mode = True
                
                outputs = model(images, modality_masks, multi_slice_mode=True)
                seg_outputs = outputs['segmentation']
                center_mask = masks[:, 2].unsqueeze(1)
                
                loss = criterion(seg_outputs, center_mask)
                dice = dice_coefficient(seg_outputs, center_mask)
            else:
                # Single-slice data
                images = batch['image'].to(device)
                masks = batch['mask'].to(device)
                modality_masks = batch['modality_mask'].to(device)
                multi_slice_mode = False
                
                outputs = model(images, modality_masks, multi_slice_mode=False)
                seg_outputs = outputs['segmentation']
                loss = criterion(seg_outputs, masks)
                dice = dice_coefficient(seg_outputs, masks)
            
            total_loss += loss.item()
            total_dice += dice
            dice_scores.append(dice)
            count += 1
    
    model.train()
    
    avg_loss = total_loss / count if count > 0 else 0
    avg_dice = total_dice / count if count > 0 else 0
    
    # Diagnostic statistics
    if len(dice_scores) > 5:
        dice_scores_sorted = sorted(dice_scores)
        median_dice = dice_scores_sorted[len(dice_scores_sorted)//2]
        min_dice = min(dice_scores)
        max_dice = max(dice_scores)
        std_dice = np.std(dice_scores)
        
        print(f"   Dice distribution: min={min_dice:.3f}, median={median_dice:.3f}, "
              f"max={max_dice:.3f}, std={std_dice:.3f}")
    
    # Classification evaluation
    cls_metrics = {}
    if enable_classification:
        cls_metrics = evaluate_classification(model, dataloader, device, num_samples)
        if cls_metrics:
            print(f"   Classification: Acc={cls_metrics['accuracy']:.3f}, "
                  f"F1={cls_metrics['f1']:.3f}, AUC={cls_metrics['auc']:.3f} "
                  f"(samples={cls_metrics['num_samples']})")
    
    return avg_loss, avg_dice, cls_metrics


# ==================== Training Tracking ====================

class IterationTracker:
    """
    Tracks training and evaluation metrics across iterations.
    
    Maintains separate metrics for:
    - Training (loss, dice per iteration)
    - Evaluation (train/val/test sets at intervals)
    - Classification (when enabled)
    
    Supports saving/loading to JSON for resuming training.
    """
    
    def __init__(self):
        # Segmentation metrics
        self.train_losses = []
        self.train_dice_scores = []
        self.eval_iterations = []
        self.eval_train_losses = []
        self.eval_train_dice_scores = []
        self.eval_val_losses = []
        self.eval_val_dice_scores = []
        self.eval_test1_losses = []
        self.eval_test1_dice_scores = []
        self.eval_test2_losses = []
        self.eval_test2_dice_scores = []
        
        # Classification metrics
        self.train_cls_acc = []
        self.eval_train_cls_metrics = []
        self.eval_val_cls_metrics = []
        self.eval_test1_cls_metrics = []
        self.eval_test2_cls_metrics = []
        
        self.iteration_counter = 0

    def add_train_result(self, loss, dice, cls_acc=0.0):
        """Record training iteration result."""
        self.train_losses.append(loss)
        self.train_dice_scores.append(dice)
        self.train_cls_acc.append(cls_acc)
        self.iteration_counter += 1

    def add_eval_results(self, train_loss, train_dice, val_loss, val_dice, 
                        test1_loss, test1_dice, test2_loss, test2_dice,
                        train_cls=None, val_cls=None, test1_cls=None, test2_cls=None):
        """Record evaluation checkpoint results."""
        self.eval_iterations.append(self.iteration_counter)
        self.eval_train_losses.append(train_loss)
        self.eval_train_dice_scores.append(train_dice)
        self.eval_val_losses.append(val_loss)
        self.eval_val_dice_scores.append(val_dice)
        self.eval_test1_losses.append(test1_loss)
        self.eval_test1_dice_scores.append(test1_dice)
        self.eval_test2_losses.append(test2_loss)
        self.eval_test2_dice_scores.append(test2_dice)
        
        self.eval_train_cls_metrics.append(train_cls or {})
        self.eval_val_cls_metrics.append(val_cls or {})
        self.eval_test1_cls_metrics.append(test1_cls or {})
        self.eval_test2_cls_metrics.append(test2_cls or {})
    
    def set_iteration_counter(self, iteration):
        """Set current iteration counter for resuming training."""
        self.iteration_counter = iteration
        
    def get_state_dict(self):
        """Get tracker state for checkpointing."""
        return {
            'train_losses': self.train_losses,
            'train_dice_scores': self.train_dice_scores,
            'train_cls_acc': self.train_cls_acc,
            'eval_iterations': self.eval_iterations,
            'eval_train_losses': self.eval_train_losses,
            'eval_train_dice_scores': self.eval_train_dice_scores,
            'eval_val_losses': self.eval_val_losses,
            'eval_val_dice_scores': self.eval_val_dice_scores,
            'eval_test1_losses': self.eval_test1_losses,
            'eval_test1_dice_scores': self.eval_test1_dice_scores,
            'eval_test2_losses': self.eval_test2_losses,
            'eval_test2_dice_scores': self.eval_test2_dice_scores,
            'eval_train_cls_metrics': self.eval_train_cls_metrics,
            'eval_val_cls_metrics': self.eval_val_cls_metrics,
            'eval_test1_cls_metrics': self.eval_test1_cls_metrics,
            'eval_test2_cls_metrics': self.eval_test2_cls_metrics,
            'iteration_counter': self.iteration_counter
        }
    
    def load_state_dict(self, state_dict):
        """Load tracker state from checkpoint."""
        self.train_losses = state_dict.get('train_losses', [])
        self.train_dice_scores = state_dict.get('train_dice_scores', [])
        self.train_cls_acc = state_dict.get('train_cls_acc', [])
        self.eval_iterations = state_dict.get('eval_iterations', [])
        self.eval_train_losses = state_dict.get('eval_train_losses', [])
        self.eval_train_dice_scores = state_dict.get('eval_train_dice_scores', [])
        self.eval_val_losses = state_dict.get('eval_val_losses', [])
        self.eval_val_dice_scores = state_dict.get('eval_val_dice_scores', [])
        self.eval_test1_losses = state_dict.get('eval_test1_losses', [])
        self.eval_test1_dice_scores = state_dict.get('eval_test1_dice_scores', [])
        self.eval_test2_losses = state_dict.get('eval_test2_losses', [])
        self.eval_test2_dice_scores = state_dict.get('eval_test2_dice_scores', [])
        self.eval_train_cls_metrics = state_dict.get('eval_train_cls_metrics', [])
        self.eval_val_cls_metrics = state_dict.get('eval_val_cls_metrics', [])
        self.eval_test1_cls_metrics = state_dict.get('eval_test1_cls_metrics', [])
        self.eval_test2_cls_metrics = state_dict.get('eval_test2_cls_metrics', [])
        self.iteration_counter = state_dict.get('iteration_counter', 0)
    
    def save_curves_to_file(self, filepath):
        """Save training curves to JSON file for visualization."""
        curves_data = {
            'train_losses': self.train_losses,
            'train_dice_scores': self.train_dice_scores,
            'train_cls_acc': self.train_cls_acc,
            'eval_iterations': self.eval_iterations,
            'eval_train_losses': self.eval_train_losses,
            'eval_train_dice_scores': self.eval_train_dice_scores,
            'eval_val_losses': self.eval_val_losses,
            'eval_val_dice_scores': self.eval_val_dice_scores,
            'eval_test1_losses': self.eval_test1_losses,
            'eval_test1_dice_scores': self.eval_test1_dice_scores,
            'eval_test2_losses': self.eval_test2_losses,
            'eval_test2_dice_scores': self.eval_test2_dice_scores,
            'eval_train_cls_metrics': self.eval_train_cls_metrics,
            'eval_val_cls_metrics': self.eval_val_cls_metrics,
            'eval_test1_cls_metrics': self.eval_test1_cls_metrics,
            'eval_test2_cls_metrics': self.eval_test2_cls_metrics,
            'iteration_counter': self.iteration_counter,
            'timestamp': datetime.now().isoformat()
        }
        
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        with open(filepath, 'w') as f:
            json.dump(curves_data, f, indent=2)
        print(f"Training curves saved to: {filepath}")
    
    def load_curves_from_file(self, filepath):
        """Load training curves from JSON file for resuming."""
        if not os.path.exists(filepath):
            print(f"Warning: Training curves file not found: {filepath}")
            return False
        
        try:
            with open(filepath, 'r') as f:
                curves_data = json.load(f)
            
            self.train_losses = curves_data.get('train_losses', [])
            self.train_dice_scores = curves_data.get('train_dice_scores', [])
            self.train_cls_acc = curves_data.get('train_cls_acc', [])
            self.eval_iterations = curves_data.get('eval_iterations', [])
            self.eval_train_losses = curves_data.get('eval_train_losses', [])
            self.eval_train_dice_scores = curves_data.get('eval_train_dice_scores', [])
            self.eval_val_losses = curves_data.get('eval_val_losses', [])
            self.eval_val_dice_scores = curves_data.get('eval_val_dice_scores', [])
            self.eval_test1_losses = curves_data.get('eval_test1_losses', [])
            self.eval_test1_dice_scores = curves_data.get('eval_test1_dice_scores', [])
            self.eval_test2_losses = curves_data.get('eval_test2_losses', [])
            self.eval_test2_dice_scores = curves_data.get('eval_test2_dice_scores', [])
            self.eval_train_cls_metrics = curves_data.get('eval_train_cls_metrics', [])
            self.eval_val_cls_metrics = curves_data.get('eval_val_cls_metrics', [])
            self.eval_test1_cls_metrics = curves_data.get('eval_test1_cls_metrics', [])
            self.eval_test2_cls_metrics = curves_data.get('eval_test2_cls_metrics', [])
            self.iteration_counter = curves_data.get('iteration_counter', 0)
            
            print(f"Training curves loaded from: {filepath}")
            print(f"   Total training iterations: {len(self.train_losses)}")
            print(f"   Total evaluation checkpoints: {len(self.eval_iterations)}")
            return True
            
        except Exception as e:
            print(f"Error loading training curves: {e}")
            return False

# ==================== Model Parameter Extraction ====================

def get_segmentation_params(model):
    """
    Extract parameter names for segmentation modules.
    
    Includes: Encoder + Decoder + Segmentation Head + Modal Completion Layer
    Excludes: Classification-specific modules (swin_transformer, feature_fusion, seg_feature_compressor)
    
    Args:
        model: MultiModal2DUNet model
    
    Returns:
        List of segmentation parameter names
    """
    seg_params = []
    for name, param in model.named_parameters():
        # Exclude classification modules but keep modal_completion
        if not (name.startswith('swin_transformer') or 
                name.startswith('feature_fusion') or 
                name.startswith('seg_feature_compressor')):
            seg_params.append(name)
    return seg_params


def get_classification_params(model):
    """
    Extract parameter names for classification modules.
    
    Includes: seg_feature_compressor + swin_transformer + feature_fusion
    
    Args:
        model: MultiModal2DUNet model
    
    Returns:
        List of classification parameter names
    """
    cls_params = []
    for name, param in model.named_parameters():
        if (name.startswith('swin_transformer') or 
            name.startswith('feature_fusion') or 
            name.startswith('seg_feature_compressor')):
            cls_params.append(name)
    return cls_params


def get_segmentation_state_dict(model):
    """
    Extract state dictionary for segmentation modules only.
    
    Args:
        model: MultiModal2DUNet model
    
    Returns:
        Filtered state dict containing only segmentation parameters
    """
    seg_param_names = get_segmentation_params(model)
    state_dict = model.state_dict()
    return {k: v for k, v in state_dict.items() if k in seg_param_names}


def get_classification_state_dict(model):
    """
    Extract state dictionary for classification modules only.
    
    Args:
        model: MultiModal2DUNet model
    
    Returns:
        Filtered state dict containing only classification parameters
    """
    cls_param_names = get_classification_params(model)
    state_dict = model.state_dict()
    return {k: v for k, v in state_dict.items() if k in cls_param_names}


def load_complete_model(model, filepath, device):
    """
    Load complete model weights from checkpoint file.
    
    Supports both new checkpoint format (with metadata) and legacy weight-only format.
    
    Args:
        model: MultiModal2DUNet model to load weights into
        filepath: Path to checkpoint file
        device: Computation device
    
    Returns:
        Boolean indicating success/failure
    """
    if not os.path.exists(filepath):
        print(f"Error: Complete model file not found: {filepath}")
        return False
        
    try:
        checkpoint = torch.load(filepath, map_location=device)
        
        # Check for new checkpoint format
        if 'model_state_dict' in checkpoint:
            print("Loading complete model format checkpoint...")
            model.load_state_dict(checkpoint['model_state_dict'])
            
            # Print loaded metadata
            if 'dice_score' in checkpoint:
                print(f"   Historical Dice score: {checkpoint['dice_score']:.4f}")
            if 'f1_score' in checkpoint:
                print(f"   Historical F1 score: {checkpoint['f1_score']:.4f}")
            if 'iteration' in checkpoint:
                print(f"   Training iteration: {checkpoint['iteration']}")
                
        else:
            # Legacy format: weights only
            print("Loading legacy format checkpoint (weights only)...")
            model.load_state_dict(checkpoint)
            
        print(f"Successfully loaded complete model: {filepath}")
        return True
        
    except Exception as e:
        print(f"Error loading complete model: {e}")
        return False


# ==================== Checkpoint Management ====================

def save_checkpoint(model, optimizer, tracker, iteration, best_dice, filepath, 
                   scheduler=None, additional_metrics=None):
    """
    Save complete training checkpoint for resuming training.
    
    Checkpoint includes:
    - Model state dict
    - Optimizer state dict
    - Training tracker state
    - Learning rate scheduler state (if provided)
    - Metadata (iteration, best dice, modalities, model config)
    
    Args:
        model: Trained model
        optimizer: Optimizer state
        tracker: IterationTracker instance
        iteration: Current training iteration
        best_dice: Best Dice score achieved
        filepath: Path to save checkpoint
        scheduler: Optional learning rate scheduler
        additional_metrics: Optional dict with extra metrics
    """
    checkpoint = {
        'iteration': iteration,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'tracker_state_dict': tracker.get_state_dict(),
        'best_dice': best_dice,
        'timestamp': datetime.now().isoformat()
    }
    
    if scheduler is not None:
        checkpoint['scheduler_state_dict'] = scheduler.state_dict()
    
    if additional_metrics is not None:
        checkpoint.update(additional_metrics)
    
    os.makedirs(os.path.dirname(filepath), exist_ok=True)
    torch.save(checkpoint, filepath)
    print(f"Checkpoint saved: {filepath}")


def load_checkpoint(model, optimizer, tracker, filepath, device):
    """
    Load training checkpoint for resuming training.
    
    Supports:
    - New format: Complete checkpoint with all training state
    - Legacy format: Model weights only
    
    Args:
        model: Model to load weights into
        optimizer: Optimizer to restore state into
        tracker: IterationTracker to restore history
        filepath: Path to checkpoint file
        device: Computation device
    
    Returns:
        (iteration, best_dice): Tuple of iteration count and best dice score
    """
    if not os.path.exists(filepath):
        print(f"Error: Checkpoint file not found: {filepath}")
        return 0, 0.0
    
    try:
        checkpoint = torch.load(filepath, map_location=device)
        
        # Check for complete checkpoint format
        if 'model_state_dict' in checkpoint:
            print("Loading complete checkpoint format...")
            model.load_state_dict(checkpoint['model_state_dict'])
            
            # Restore optimizer state
            if 'optimizer_state_dict' in checkpoint:
                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
                # Move optimizer state to correct device
                for state in optimizer.state.values():
                    for k, v in state.items():
                        if isinstance(v, torch.Tensor):
                            state[k] = v.to(device)
            
            # Restore tracker state
            if 'tracker_state_dict' in checkpoint:
                tracker.load_state_dict(checkpoint['tracker_state_dict'])
            
            iteration = checkpoint.get('iteration', 0)
            best_dice = checkpoint.get('best_dice', 0.0)
            
            print(f"Successfully loaded checkpoint: {filepath}")
            print(f"   Resuming from iteration {iteration}")
            print(f"   Previous best Dice: {best_dice:.4f}")
            print(f"   Training history: {len(tracker.train_losses)} training samples")
            print(f"   Evaluation history: {len(tracker.eval_iterations)} evaluations")
            
            return iteration, best_dice
            
        else:
            # Legacy format: weights only
            print("Loading legacy checkpoint format (weights only)...")
            model.load_state_dict(checkpoint)
            
            # Try to extract iteration from filename
            import re
            iteration_match = re.search(r'iter_(\d+)', filepath)
            if iteration_match:
                iteration = int(iteration_match.group(1))
                print(f"   Inferred iteration from filename: {iteration}")
            else:
                iteration = 0
                print("   Could not infer iteration, starting from 0")
            
            tracker.set_iteration_counter(iteration)
            
            print(f"Successfully loaded legacy checkpoint: {filepath}")
            print(f"   Resuming from iteration {iteration}")
            print(f"   Warning: Optimizer state and training history will be reset")
            
            return iteration, 0.0
        
    except Exception as e:
        print(f"Error loading checkpoint: {e}")
        import traceback
        traceback.print_exc()
        return 0, 0.0


# ==================== Visualization Functions ====================

def visualize_multimodal_predictions(model, train_loader, val_loader, test1_loader, test2_loader, 
                                    tracker, device, modalities):
    """
    Generate comprehensive visualization of training progress and segmentation results.
    
    Creates three figures:
    1. Performance dashboard (3x2 layout): Loss, Dice, classification metrics
    2. Segmentation samples: Visual comparison of predictions vs ground truth
    3. Class-specific metrics: Per-class classification performance
    
    Args:
        model: Trained model
        train_loader: Training data loader
        val_loader: Validation data loader
        test1_loader: First test data loader
        test2_loader: Second test data loader
        tracker: IterationTracker with metrics history
        device: Computation device
        modalities: List of modality names (e.g., ['T1', 'T2', 'T1C'])
    """
    
    # Figure 1: Performance Dashboard (3x2 layout)
    fig1 = plt.figure(figsize=(20, 15))
    fig1.suptitle(f'Joint Training Performance Dashboard - Iteration {tracker.iteration_counter}', 
                  fontsize=16, fontweight='bold')
    
    # Row 1, Col 1: Training Loss
    ax1 = plt.subplot(3, 2, 1)
    if len(tracker.train_losses) > 0:
        ax1.plot(tracker.train_losses, alpha=0.6, color='lightblue', label='Raw Loss')
        if len(tracker.train_losses) > 20:
            smoothed = np.convolve(tracker.train_losses, np.ones(20)/20, mode='valid')
            ax1.plot(np.arange(19, len(tracker.train_losses)), smoothed, 'b-', label='Smoothed')
    ax1.set_title('Training Loss (per Iteration)')
    ax1.set_xlabel('Iteration')
    ax1.set_ylabel('Loss')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # Row 1, Col 2: Training Dice Score
    ax2 = plt.subplot(3, 2, 2)
    if len(tracker.train_dice_scores) > 0:
        ax2.plot(tracker.train_dice_scores, alpha=0.6, color='lightgreen', label='Raw Dice')
        if len(tracker.train_dice_scores) > 20:
            smoothed = np.convolve(tracker.train_dice_scores, np.ones(20)/20, mode='valid')
            ax2.plot(np.arange(19, len(tracker.train_dice_scores)), smoothed, 'g-', label='Smoothed')
    ax2.set_title('Training Dice Score (per Iteration)')
    ax2.set_xlabel('Iteration')
    ax2.set_ylabel('Dice Score')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # Row 2, Col 1: Dice Comparison Across Datasets
    ax3 = plt.subplot(3, 2, 3)
    if len(tracker.eval_iterations) > 0:
        ax3.plot(tracker.eval_iterations, tracker.eval_train_dice_scores, 'b-o', 
                markersize=4, label='Train All')
        ax3.plot(tracker.eval_iterations, tracker.eval_val_dice_scores, 'r-s', 
                markersize=4, label='Validation')
        if test1_loader:
            ax3.plot(tracker.eval_iterations, tracker.eval_test1_dice_scores, 'm-^', 
                    markersize=4, label='Test1')
        if test2_loader:
            ax3.plot(tracker.eval_iterations, tracker.eval_test2_dice_scores, 'g-d', 
                    markersize=4, label='Test2')
    ax3.set_title('Evaluation Dice Scores Comparison')
    ax3.set_xlabel('Iteration')
    ax3.set_ylabel('Dice Score')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    ax3.set_ylim(0, 1)
    
    # Row 2, Col 2: Loss Comparison Across Datasets
    ax4 = plt.subplot(3, 2, 4)
    if len(tracker.eval_iterations) > 0:
        ax4.plot(tracker.eval_iterations, tracker.eval_train_losses, 'b-o', 
                markersize=4, label='Train All')
        ax4.plot(tracker.eval_iterations, tracker.eval_val_losses, 'r-s', 
                markersize=4, label='Validation')
        if test1_loader:
            ax4.plot(tracker.eval_iterations, tracker.eval_test1_losses, 'm-^', 
                    markersize=4, label='Test1')
        if test2_loader:
            ax4.plot(tracker.eval_iterations, tracker.eval_test2_losses, 'g-d', 
                    markersize=4, label='Test2')
    ax4.set_title('Evaluation Loss Comparison (Every 2000 Iterations)')
    ax4.set_xlabel('Iteration')
    ax4.set_ylabel('Average Loss')
    ax4.legend()
    ax4.grid(True, alpha=0.3)
    
    # Row 3, Col 1: Classification Accuracy
    ax5 = plt.subplot(3, 2, 5)
    if len(tracker.eval_iterations) > 0:
        train_acc_scores = [metrics.get('accuracy', 0) for metrics in tracker.eval_train_cls_metrics]
        val_acc_scores = [metrics.get('accuracy', 0) for metrics in tracker.eval_val_cls_metrics]
        test1_acc_scores = [metrics.get('accuracy', 0) for metrics in tracker.eval_test1_cls_metrics]
        test2_acc_scores = [metrics.get('accuracy', 0) for metrics in tracker.eval_test2_cls_metrics]
        
        ax5.plot(tracker.eval_iterations, train_acc_scores, 'b-o', markersize=4, label='Train All ACC')
        ax5.plot(tracker.eval_iterations, val_acc_scores, 'r-s', markersize=4, label='Validation ACC')
        if test1_loader and any(score > 0 for score in test1_acc_scores):
            ax5.plot(tracker.eval_iterations, test1_acc_scores, 'm-^', markersize=4, label='Test1 ACC')
        if test2_loader and any(score > 0 for score in test2_acc_scores):
            ax5.plot(tracker.eval_iterations, test2_acc_scores, 'g-d', markersize=4, label='Test2 ACC')
        
        # Add real-time training accuracy
        if len(tracker.train_cls_acc) > 0:
            train_acc_downsampled = []
            eval_points = []
            for i, eval_iter in enumerate(tracker.eval_iterations):
                if eval_iter < len(tracker.train_cls_acc):
                    start_idx = max(0, eval_iter - 500)
                    end_idx = min(len(tracker.train_cls_acc), eval_iter + 500)
                    avg_acc = np.mean(tracker.train_cls_acc[start_idx:end_idx])
                    train_acc_downsampled.append(avg_acc)
                    eval_points.append(eval_iter)
            if train_acc_downsampled:
                ax5.plot(eval_points, train_acc_downsampled, 'c--', alpha=0.7, 
                        label='Real-time Train ACC')
                
    ax5.set_title('Classification Accuracy Comparison')
    ax5.set_xlabel('Iteration')
    ax5.set_ylabel('Accuracy')
    ax5.legend()
    ax5.grid(True, alpha=0.3)
    ax5.set_ylim(0, 1)
    
    # Row 3, Col 2: F1 and AUC Scores
    ax6 = plt.subplot(3, 2, 6)
    if len(tracker.eval_iterations) > 0:
        # F1 scores
        train_f1_scores = [metrics.get('f1', 0) for metrics in tracker.eval_train_cls_metrics]
        val_f1_scores = [metrics.get('f1', 0) for metrics in tracker.eval_val_cls_metrics]
        test1_f1_scores = [metrics.get('f1', 0) for metrics in tracker.eval_test1_cls_metrics]
        test2_f1_scores = [metrics.get('f1', 0) for metrics in tracker.eval_test2_cls_metrics]
        
        # AUC scores
        train_auc_scores = [metrics.get('auc', 0) for metrics in tracker.eval_train_cls_metrics]
        val_auc_scores = [metrics.get('auc', 0) for metrics in tracker.eval_val_cls_metrics]
        test1_auc_scores = [metrics.get('auc', 0) for metrics in tracker.eval_test1_cls_metrics]
        test2_auc_scores = [metrics.get('auc', 0) for metrics in tracker.eval_test2_cls_metrics]
        
        # Plot F1 scores (solid lines)
        ax6.plot(tracker.eval_iterations, train_f1_scores, 'b-o', markersize=3, label='Train All F1')
        ax6.plot(tracker.eval_iterations, val_f1_scores, 'r-s', markersize=3, label='Validation F1')
        if test1_loader and any(score > 0 for score in test1_f1_scores):
            ax6.plot(tracker.eval_iterations, test1_f1_scores, 'm-^', markersize=3, label='Test1 F1')
        if test2_loader and any(score > 0 for score in test2_f1_scores):
            ax6.plot(tracker.eval_iterations, test2_f1_scores, 'g-d', markersize=3, label='Test2 F1')
        
        # Plot AUC scores (dashed lines)
        ax6.plot(tracker.eval_iterations, train_auc_scores, 'b--', alpha=0.8, label='Train All AUC')
        ax6.plot(tracker.eval_iterations, val_auc_scores, 'r--', alpha=0.8, label='Validation AUC')
        if test1_loader and any(score > 0 for score in test1_auc_scores):
            ax6.plot(tracker.eval_iterations, test1_auc_scores, 'm--', alpha=0.8, label='Test1 AUC')
        if test2_loader and any(score > 0 for score in test2_auc_scores):
            ax6.plot(tracker.eval_iterations, test2_auc_scores, 'g--', alpha=0.8, label='Test2 AUC')
    
    ax6.set_title('Classification F1 & AUC Comparison')
    ax6.set_xlabel('Iteration')
    ax6.set_ylabel('F1 Score / AUC')
    ax6.legend()
    ax6.grid(True, alpha=0.3)
    ax6.set_ylim(0, 1)
    
    fig1.tight_layout(rect=[0, 0, 1, 0.96])
    plt.show()
    
    # Figure 3: Class-specific Accuracy (if classification enabled)
    if len(tracker.eval_iterations) > 0:
        fig3 = plt.figure(figsize=(12, 6))
        fig3.suptitle(f'Class-specific Classification Performance - Iteration {tracker.iteration_counter}', 
                     fontsize=14, fontweight='bold')
        
        # Left subplot: PCR=0 accuracy
        ax7 = plt.subplot(1, 2, 1)
        class0_acc_train = [metrics.get('class_0_acc', 0) for metrics in tracker.eval_train_cls_metrics]
        class0_acc_val = [metrics.get('class_0_acc', 0) for metrics in tracker.eval_val_cls_metrics]
        class0_acc_test1 = [metrics.get('class_0_acc', 0) for metrics in tracker.eval_test1_cls_metrics]
        class0_acc_test2 = [metrics.get('class_0_acc', 0) for metrics in tracker.eval_test2_cls_metrics]
        
        if any(acc > 0 for acc in class0_acc_train + class0_acc_val):
            ax7.plot(tracker.eval_iterations, class0_acc_train, 'b-o', markersize=4, label='Train All')
            ax7.plot(tracker.eval_iterations, class0_acc_val, 'r-s', markersize=4, label='Validation')
            if test1_loader and any(acc > 0 for acc in class0_acc_test1):
                ax7.plot(tracker.eval_iterations, class0_acc_test1, 'm-^', markersize=4, label='Test1')
            if test2_loader and any(acc > 0 for acc in class0_acc_test2):
                ax7.plot(tracker.eval_iterations, class0_acc_test2, 'g-d', markersize=4, label='Test2')
        
        ax7.set_title('PCR = 0 (Non-responder) Classification Accuracy')
        ax7.set_xlabel('Iteration')
        ax7.set_ylabel('Class-specific Accuracy')
        ax7.legend()
        ax7.grid(True, alpha=0.3)
        ax7.set_ylim(0, 1)
        
        # Right subplot: PCR=1 accuracy
        ax8 = plt.subplot(1, 2, 2)
        class1_acc_train = [metrics.get('class_1_acc', 0) for metrics in tracker.eval_train_cls_metrics]
        class1_acc_val = [metrics.get('class_1_acc', 0) for metrics in tracker.eval_val_cls_metrics]
        class1_acc_test1 = [metrics.get('class_1_acc', 0) for metrics in tracker.eval_test1_cls_metrics]
        class1_acc_test2 = [metrics.get('class_1_acc', 0) for metrics in tracker.eval_test2_cls_metrics]
        
        if any(acc > 0 for acc in class1_acc_train + class1_acc_val):
            ax8.plot(tracker.eval_iterations, class1_acc_train, 'b-o', markersize=4, label='Train All')
            ax8.plot(tracker.eval_iterations, class1_acc_val, 'r-s', markersize=4, label='Validation')
            if test1_loader and any(acc > 0 for acc in class1_acc_test1):
                ax8.plot(tracker.eval_iterations, class1_acc_test1, 'm-^', markersize=4, label='Test1')
            if test2_loader and any(acc > 0 for acc in class1_acc_test2):
                ax8.plot(tracker.eval_iterations, class1_acc_test2, 'g-d', markersize=4, label='Test2')
        
        ax8.set_title('PCR = 1 (Responder) Classification Accuracy')
        ax8.set_xlabel('Iteration')
        ax8.set_ylabel('Class-specific Accuracy')
        ax8.legend()
        ax8.grid(True, alpha=0.3)
        ax8.set_ylim(0, 1)
        
        fig3.tight_layout(rect=[0, 0, 1, 0.96])
        plt.show()
    
    # Figure 2: Segmentation Results
    NUM_SAMPLES_PER_DATASET = 2
    NUM_COLS = len(modalities) + 2
    NUM_ROWS = 4 * NUM_SAMPLES_PER_DATASET
    fig2 = plt.figure(figsize=(NUM_COLS * 4, NUM_ROWS * 3.0))
    fig2.suptitle(f'Segmentation Samples - Iteration {tracker.iteration_counter}', 
                  fontsize=16, fontweight='bold')
    
    datasets_to_vis = [('Train All', train_loader), ('Validation', val_loader), 
                       ('Test1', test1_loader), ('Test2', test2_loader)]
    
    model.eval()
    with torch.no_grad():
        current_row = 0
        for dataset_name, dataloader in datasets_to_vis:
            if not dataloader or not dataloader.dataset:
                current_row += NUM_SAMPLES_PER_DATASET
                continue
            
            for sample_vis_idx in range(NUM_SAMPLES_PER_DATASET):
                try:
                    random_idx = random.randint(0, len(dataloader.dataset) - 1)
                    sample = dataloader.dataset[random_idx]
                    
                    # Handle different data formats
                    if 'images' in sample:
                        image = sample['images'][2]
                        mask = sample['masks'][2]
                        modality_mask_np = sample['modality_mask'].numpy()
                    else:
                        image = sample['image']
                        mask = sample['mask']
                        modality_mask_np = sample['modality_mask'].numpy()
                    
                    image = image.unsqueeze(0).to(device)
                    mask = mask.unsqueeze(0).to(device)
                    modality_mask = sample['modality_mask'].unsqueeze(0).to(device)
                    
                    # Forward pass
                    outputs = model(image, modality_mask, multi_slice_mode=False)
                    
                    if isinstance(outputs, dict):
                        seg_outputs = outputs['segmentation']
                    else:
                        seg_outputs = outputs
                    
                    pred = torch.sigmoid(seg_outputs)
                    
                    # Select background image based on available modality
                    base_img = None
                    for mod_idx, modality in enumerate(modalities):
                        if modality_mask_np[mod_idx] == 1:
                            base_img = image[0, mod_idx].cpu().numpy()
                            break
                    
                    if base_img is None:
                        base_img = image[0, 0].cpu().numpy()
                    
                    # Display all modalities
                    for mod_idx, modality in enumerate(modalities):
                        ax = plt.subplot(NUM_ROWS, NUM_COLS, current_row * NUM_COLS + mod_idx + 1)
                        img_slice = image[0, mod_idx].cpu().numpy()
                        ax.imshow(img_slice, cmap='gray')
                        
                        title = f'{dataset_name} Sample {sample_vis_idx+1} - {modality}'
                        if modality_mask_np[mod_idx] == 0:
                            title += ' (Missing)'
                            ax.text(0.5, 0.5, 'Missing', transform=ax.transAxes,
                                   ha='center', va='center', color='red', fontsize=16, weight='bold')
                        
                        ax.set_title(title, fontsize=10)
                        ax.axis('off')
                    
                    # Plot ground truth
                    gt_ax = plt.subplot(NUM_ROWS, NUM_COLS, current_row * NUM_COLS + len(modalities) + 1)
                    mask_data = mask[0, 0].cpu().numpy()
                    gt_ax.imshow(base_img, cmap='gray')
                    
                    if mask_data.max() > 0:
                        import cv2
                        contours, _ = cv2.findContours((mask_data * 255).astype(np.uint8), 
                                                      cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                        for c in contours:
                            if len(c) > 2:
                                gt_ax.plot(c[:, 0, 0], c[:, 0, 1], 'g-', linewidth=2)
                    
                    sample_id = sample.get("sample_id", f"Sample_{random_idx}")
                    if isinstance(sample_id, str) and len(sample_id) > 12:
                        sample_id = sample_id[:12]
                    gt_ax.set_title(f'Ground Truth\nID: {sample_id}', fontsize=10)
                    gt_ax.axis('off')
                    
                    # Plot prediction
                    pred_ax = plt.subplot(NUM_ROWS, NUM_COLS, current_row * NUM_COLS + len(modalities) + 2)
                    pred_data = pred[0, 0].cpu().numpy()
                    pred_ax.imshow(base_img, cmap='gray')
                    
                    pred_binary = (pred_data > 0.5).astype(np.uint8)
                    if pred_binary.max() > 0:
                        import cv2
                        contours, _ = cv2.findContours(pred_binary * 255, 
                                                      cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                        for c in contours:
                            if len(c) > 2:
                                pred_ax.plot(c[:, 0, 0], c[:, 0, 1], 'r--', linewidth=2)
                    
                    dice_score = dice_coefficient(seg_outputs, mask)
                    pred_ax.set_title(f'Model Prediction\nDice: {dice_score:.3f}', fontsize=10)
                    pred_ax.axis('off')
                    
                except Exception as e:
                    print(f"Visualization failed for {dataset_name} sample {sample_vis_idx}: {e}")
                finally:
                    current_row += 1
    
    fig2.tight_layout(rect=[0, 0, 1, 0.96])
    plt.show()
    model.train()


# ==================== Batch Monitoring ====================

def log_batch_class_distribution(pcr_labels, iteration):
    """
    Log class distribution in current batch.
    
    Provides periodic monitoring of class balance in training batches.
    
    Args:
        pcr_labels: [B] tensor of PCR labels (0 or 1)
        iteration: Current training iteration
    """
    if pcr_labels is None:
        return
    
    pcr_labels_np = pcr_labels.cpu().numpy()
    class_counts = np.bincount(pcr_labels_np.astype(int), minlength=2)
    
    if iteration % 50 == 0:
        print(f"Iteration {iteration} - Class distribution: PCR=0: {class_counts[0]}, PCR=1: {class_counts[1]}")


def check_class_collapse(predictions, targets, iteration):
    """
    Detect and warn about class collapse in predictions.
    
    Class collapse occurs when model predicts only one class,
    indicating potential training instability.
    
    Args:
        predictions: [B] or [B, 1] predicted probabilities
        targets: [B] or [B, 1] ground truth labels
        iteration: Current training iteration
    
    Returns:
        Boolean indicating whether class collapse detected
    """
    if predictions is None or targets is None:
        return False
    
    pred_classes = (predictions > 0.5).astype(int)
    unique_preds = np.unique(pred_classes)
    
    # Warn if model predicts only one class
    if len(unique_preds) == 1:
        collapsed_class = unique_preds[0]
        print(f"Warning: Class collapse detected at iteration {iteration}! "
              f"Model predicting only class {collapsed_class}")
        return True
    
    return False

# ==================== Main Training ====================

def main():
    """
    Main entry point for 2D multimodal segmentation and classification training.
    
    Initializes:
    - Dataset loaders (training, validation, test sets)
    - Model architecture with optional classification branch
    - Loss functions and optimizer
    - Training loop with periodic evaluation and visualization
    
    Supports:
    - Checkpoint resumption
    - Module freezing (segmentation/classification)
    - Multi-dataset training curves visualization
    - Complete model saving with metadata
    """
    
    print("=" * 70)
    print("Initializing 2D Multimodal Segmentation + Classification Joint System")
    print("=" * 70)
    
    print("\nTraining Configuration:")
    print(f"  Freeze Segmentation Module: {FREEZE_SEGMENTATION}")
    print(f"  Freeze Classification Module: {FREEZE_CLASSIFICATION}")
    print(f"  Complete Model Path: {LOAD_COMPLETE_MODEL if LOAD_COMPLETE_MODEL else 'None'}")
    print(f"  Segmentation Model Path: {LOAD_SEGMENTATION_MODEL if LOAD_SEGMENTATION_MODEL else 'None'}")
    print(f"  Classification Model Path: {LOAD_CLASSIFICATION_MODEL if LOAD_CLASSIFICATION_MODEL else 'None'}")
    print(f"  Resume from Checkpoint: {'Yes' if RESUME_CHECKPOINT else 'No'}")
    print(f"  Training Curves File: {TRAINING_CURVES_FILE}\n")
    
    os.makedirs(LOG_DIR, exist_ok=True)
    os.makedirs(SAVE_DIR, exist_ok=True)
    
    # Load PCR clinical labels
    id_to_pcr = None
    if ENABLE_CLASSIFICATION and os.path.exists(CLINICAL_FILE):
        print("Loading PCR clinical labels...")
        id_to_pcr = load_clinical_labels(CLINICAL_FILE)
    else:
        print("Warning: Classification disabled or clinical file not found. Performing segmentation training only.")
    
    # Create datasets
    print("Loading 2D multimodal datasets...")
    train_dataset = MultiModal2DDataset(
        TRAIN_DIR, IMG_SIZE, MODALITIES, id_to_pcr, 
        is_training=True
    )
    
    val_dataset = MultiModal2DDataset(
        VAL_DIR, IMG_SIZE, MODALITIES, id_to_pcr, 
        is_training=False, use_max_slice_only=True, use_multi_slice=True
    )
    
    test1_dataset = None
    if os.path.exists(TEST_DIR1):
        test1_dataset = MultiModal2DDataset(
            TEST_DIR1, IMG_SIZE, MODALITIES, id_to_pcr, 
            is_training=False, use_max_slice_only=True, use_multi_slice=True
        )
    
    test2_dataset = None
    if os.path.exists(TEST_DIR2):
        test2_dataset = MultiModal2DDataset(
            TEST_DIR2, IMG_SIZE, MODALITIES, id_to_pcr, 
            is_training=False, use_max_slice_only=True, use_multi_slice=True
        )
    
    if len(train_dataset.samples) == 0:
        print("Error: Training dataset is empty!")
        return
    
    # Create data loaders with optimized settings
    pin_memory = (DEVICE.type == 'cuda')
    train_loader = DataLoader(
        train_dataset,
        batch_size=BATCH_SIZE,
        shuffle=True,
        num_workers=NUM_WORKERS,
        pin_memory=pin_memory,
        persistent_workers=(NUM_WORKERS > 0)
    )
    
    val_loader = DataLoader(
        val_dataset, batch_size=BATCH_SIZE, shuffle=False,
        num_workers=NUM_WORKERS, pin_memory=pin_memory,
        persistent_workers=(NUM_WORKERS > 0)
    )
    
    test1_loader = None
    if test1_dataset:
        test1_loader = DataLoader(
            test1_dataset, batch_size=BATCH_SIZE, shuffle=False,
            num_workers=NUM_WORKERS, pin_memory=pin_memory,
            persistent_workers=(NUM_WORKERS > 0)
        )
    
    test2_loader = None
    if test2_dataset:
        test2_loader = DataLoader(
            test2_dataset, batch_size=BATCH_SIZE, shuffle=False,
            num_workers=NUM_WORKERS, pin_memory=pin_memory,
            persistent_workers=(NUM_WORKERS > 0)
        )
    
    # Create model
    print("Creating 2D multimodal U-Net with joint classification...")
    model = MultiModal2DUNet(
        MODALITIES, OUT_CHANNELS, BASE_FEATURES,
        enable_classification=ENABLE_CLASSIFICATION
    ).to(DEVICE)
    
    # ==================== Model Loading Logic ====================
    model_loaded = False
    
    # Priority 1: Load complete model (recommended)
    if LOAD_COMPLETE_MODEL:
        print(f"Attempting to load complete model: {LOAD_COMPLETE_MODEL}")
        if load_complete_model(model, LOAD_COMPLETE_MODEL, DEVICE):
            model_loaded = True
            print("Complete model loaded successfully")
        else:
            print("Complete model loading failed, attempting separate module loading")
    
    # Priority 2: Load segmentation and classification modules separately
    if not model_loaded:
        # Load segmentation model
        if LOAD_SEGMENTATION_MODEL and os.path.exists(LOAD_SEGMENTATION_MODEL):
            print(f"Loading segmentation model: {LOAD_SEGMENTATION_MODEL}")
            checkpoint = torch.load(LOAD_SEGMENTATION_MODEL, map_location=DEVICE)
            
            seg_param_names = get_segmentation_params(model)
            model_dict = model.state_dict()
            
            if 'model_state_dict' in checkpoint:
                seg_dict = {
                    k: v for k, v in checkpoint['model_state_dict'].items()
                    if k in model_dict and k in seg_param_names
                }
            else:
                seg_dict = {
                    k: v for k, v in checkpoint.items()
                    if k in model_dict and k in seg_param_names
                }
            
            model_dict.update(seg_dict)
            model.load_state_dict(model_dict)
            print(f"Successfully loaded {len(seg_dict)} segmentation parameters")
        
        # Load classification model
        if LOAD_CLASSIFICATION_MODEL and os.path.exists(LOAD_CLASSIFICATION_MODEL):
            print(f"Loading classification model: {LOAD_CLASSIFICATION_MODEL}")
            checkpoint = torch.load(LOAD_CLASSIFICATION_MODEL, map_location=DEVICE)
            
            cls_param_names = get_classification_params(model)
            model_dict = model.state_dict()
            
            if 'model_state_dict' in checkpoint:
                cls_dict = {
                    k: v for k, v in checkpoint['model_state_dict'].items()
                    if k in model_dict and k in cls_param_names
                }
            else:
                cls_dict = {
                    k: v for k, v in checkpoint.items()
                    if k in model_dict and k in cls_param_names
                }
            
            model_dict.update(cls_dict)
            model.load_state_dict(model_dict)
            print(f"Successfully loaded {len(cls_dict)} classification parameters")
    
    # ==================== Module Freezing Logic ====================
    seg_param_names = get_segmentation_params(model)
    cls_param_names = get_classification_params(model)
    
    if FREEZE_SEGMENTATION:
        print("Freezing segmentation module...")
        frozen_count = 0
        for name, param in model.named_parameters():
            if name in seg_param_names:
                param.requires_grad = False
                frozen_count += 1
        print(f"Froze {frozen_count} segmentation parameters")
    
    if FREEZE_CLASSIFICATION:
        print("Freezing classification module...")
        frozen_count = 0
        for name, param in model.named_parameters():
            if name in cls_param_names:
                param.requires_grad = False
                frozen_count += 1
        print(f"Froze {frozen_count} classification parameters")
    
    # ==================== Model Size Statistics ====================
    def get_model_size(model):
        """
        Calculate model size statistics.
        
        Returns:
            (size_mb, param_count, buffer_count): Model size in MB and counts
        """
        param_size = 0
        param_count = 0
        for param in model.parameters():
            param_count += param.numel()
            param_size += param.numel() * param.element_size()
        
        buffer_size = 0
        buffer_count = 0
        for buffer in model.buffers():
            buffer_count += buffer.numel()
            buffer_size += buffer.numel() * buffer.element_size()
        
        size_all_mb = (param_size + buffer_size) / 1024 ** 2
        return size_all_mb, param_count, buffer_count
    
    model_size_mb, total_params, total_buffers = get_model_size(model)
    print("\nModel Size Statistics:")
    print(f"  Total Parameters: {total_params:,}")
    print(f"  Total Buffers: {total_buffers:,}")
    print(f"  Model Size: {model_size_mb:.2f} MB")
    print(f"  Checkpoint File Size (estimated): {model_size_mb:.2f} MB")
    
    # ==================== Loss Functions ====================
    seg_criterion = CombinedLossWithReconstruction()
    
    cls_criterion = None
    if ENABLE_CLASSIFICATION:
        cls_criterion = nn.BCEWithLogitsLoss()
        print("Classification Loss: Binary Cross Entropy with Logits")
    
    print(f"\nRegularization Configuration:")
    print(f"  Weight Decay: {WEIGHT_DECAY}")
    print(f"  Gradient Clip Value: {GRADIENT_CLIP_VALUE}")
    
    # ==================== Optimizer Configuration ====================
    seg_params = []
    cls_params = []
    
    for name, param in model.named_parameters():
        if not param.requires_grad:
            continue
        if name in cls_param_names:
            cls_params.append(param)
        elif name in seg_param_names:
            seg_params.append(param)
    
    optimizer_param_groups = []
    if seg_params:
        optimizer_param_groups.append({'params': seg_params, 'lr': SEGMENTATION_LR})
    if cls_params:
        optimizer_param_groups.append({'params': cls_params, 'lr': CLASSIFICATION_LR})
    
    if not optimizer_param_groups:
        raise ValueError("No trainable parameters found! Check freezing settings.")
    
    optimizer = optim.AdamW(optimizer_param_groups, weight_decay=WEIGHT_DECAY)
    
    print("\nOptimizer Configuration:")
    print(f"  Segmentation Parameters: {len(seg_params)} (LR: {SEGMENTATION_LR})")
    print(f"  Classification Parameters: {len(cls_params)} (LR: {CLASSIFICATION_LR})")
    
    # Learning rate scheduler
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='max', factor=0.7,
        patience=50, min_lr=1e-6
    )
    
    tracker = IterationTracker()
    
    # Load training curves if continuing training
    if CONTINUE_TRAINING:
        print("\nLoading historical training curves...")
        tracker.load_curves_from_file(TRAINING_CURVES_FILE)
    
    print(f"Total Model Parameters: {sum(p.numel() for p in model.parameters()):,}")
    
    # ==================== Checkpoint Loading ====================
    start_iteration = 0
    best_dice = 0.0
    
    if RESUME_CHECKPOINT and os.path.exists(RESUME_CHECKPOINT):
        print(f"\nResuming from checkpoint...")
        start_iteration, best_dice = load_checkpoint(model, optimizer, tracker, RESUME_CHECKPOINT, DEVICE)
        print(f"Training will continue until iteration {MAX_ITERATIONS}")
    else:
        if RESUME_CHECKPOINT:
            print(f"Warning: Checkpoint file not found: {RESUME_CHECKPOINT}")
        print("Starting training from scratch...")
    
    print(f"Training Range: Iteration {start_iteration + 1} to {MAX_ITERATIONS}")
    
    # ==================== Training Loop ====================
    def infinite_dataloader(dataloader):
        """
        Create infinite data loader for continuous training.
        
        Args:
            dataloader: PyTorch DataLoader
        
        Yields:
            Batches cyclically
        """
        while True:
            for batch in dataloader:
                yield batch
    
    train_iter = infinite_dataloader(train_loader)
    
    remaining_iterations = MAX_ITERATIONS - start_iteration
    progress_bar = tqdm(
        range(remaining_iterations),
        desc=f"2D Multimodal Training (iter {start_iteration + 1}-{MAX_ITERATIONS})",
        total=remaining_iterations
    )
    
    for iteration in progress_bar:
        real_iteration = start_iteration + iteration
        
        try:
            model.train()
            batch = next(train_iter)
            
            # Use single-slice mode during training
            images = batch['image'].to(DEVICE)
            masks = batch['mask'].to(DEVICE)
            modality_masks = batch['modality_mask'].to(DEVICE)
            pcr_labels = batch['pcr_label'].to(DEVICE) if ENABLE_CLASSIFICATION else None
            has_pcr = batch['has_pcr'] if ENABLE_CLASSIFICATION else None
            multi_slice_mode = False
            
            # Log batch statistics periodically
            if ENABLE_CLASSIFICATION and iteration % 200 == 0:
                if pcr_labels is not None:
                    pcr_count = (pcr_labels.cpu().numpy() == 1).sum()
                    print(f"Batch {iteration}: PCR=1 samples: {pcr_count}/{len(pcr_labels)}")
            
            optimizer.zero_grad()
            
            # Determine whether to compute classification this iteration
            enable_cls_this_iteration = (ENABLE_CLASSIFICATION and len(cls_params) > 0)
            
            # Random modality dropout (consistency training)
            if random.random() < 0.3:
                modified_mask = modality_masks.clone()
                for b in range(modality_masks.shape[0]):
                    available_modalities = torch.where(modified_mask[b] == 1)[0]
                    if len(available_modalities) > 1:
                        drop_idx = available_modalities[random.randint(0, len(available_modalities) - 1)]
                        modified_mask[b, drop_idx] = 0
                        images[b, drop_idx] = 0
                
                if enable_cls_this_iteration:
                    outputs = model(
                        images, modified_mask, return_features=True,
                        return_classification=True, multi_slice_mode=multi_slice_mode
                    )
                else:
                    outputs = model(
                        images, modified_mask, return_features=True,
                        return_classification=False, multi_slice_mode=multi_slice_mode
                    )
            else:
                if enable_cls_this_iteration:
                    outputs = model(
                        images, modality_masks, return_features=True,
                        return_classification=True, multi_slice_mode=multi_slice_mode
                    )
                else:
                    outputs = model(
                        images, modality_masks, return_features=True,
                        return_classification=False, multi_slice_mode=multi_slice_mode
                    )
            
            # Extract outputs
            seg_outputs = outputs['segmentation']
            cls_outputs = outputs.get('classification', None)
            reconstruction_targets = outputs.get('reconstruction_targets', None)
            complete_features = outputs.get('complete_features', None)
            
            # ==================== Loss Computation ====================
            total_loss = 0
            
            # Segmentation loss (all samples)
            seg_loss = seg_criterion(seg_outputs, masks, reconstruction_targets, complete_features)
            total_loss += seg_loss
            
            # Classification loss (samples with PCR labels)
            current_cls_acc = 0.0
            if ENABLE_CLASSIFICATION and cls_outputs is not None and len(cls_params) > 0:
                pcr_indices = [i for i, has in enumerate(has_pcr) if has]
                if pcr_indices:
                    cls_logits = cls_outputs[pcr_indices]
                    cls_targets = pcr_labels[pcr_indices]
                    cls_loss = cls_criterion(cls_logits.squeeze(), cls_targets)
                    total_loss += 1.0 * cls_loss
                    
                    # Compute classification accuracy
                    with torch.no_grad():
                        cls_probs = torch.sigmoid(cls_logits).cpu().numpy().flatten()
                        cls_preds = (cls_probs > 0.5).astype(int)
                        cls_true = cls_targets.cpu().numpy()
                        current_cls_acc = np.mean(cls_preds == cls_true)
                        
                        # Detect class collapse
                        if iteration % 100 == 0:
                            check_class_collapse(cls_probs, cls_true, real_iteration)
            
            total_loss.backward()
            optimizer.step()
            
            # Clear GPU cache periodically
            if DEVICE.type == 'cuda' and (real_iteration + 1) % 50 == 0:
                torch.cuda.empty_cache()
            
            # ==================== Metrics Recording ====================
            current_loss = total_loss.item()
            current_dice = dice_coefficient(seg_outputs, masks)
            tracker.add_train_result(current_loss, current_dice, current_cls_acc)
            
            postfix_dict = {
                'Iter': f'{real_iteration + 1}',
                'Loss': f'{current_loss:.4f}',
                'Dice': f'{current_dice:.4f}',
                'LR': f'{optimizer.param_groups[0]["lr"]:.2e}',
                'Best': f'{best_dice:.3f}'
            }
            
            # Display training mode
            if len(seg_params) > 0 and len(cls_params) > 0:
                postfix_dict['Mode'] = 'Joint'
            elif len(seg_params) > 0:
                postfix_dict['Mode'] = 'Seg'
            elif len(cls_params) > 0:
                postfix_dict['Mode'] = 'Cls'
            
            if ENABLE_CLASSIFICATION and current_cls_acc > 0:
                postfix_dict['ClsAcc'] = f'{current_cls_acc:.3f}'
            
            progress_bar.set_postfix(postfix_dict)
            
            # ==================== Periodic Evaluation ====================
            if (real_iteration + 1) % EVAL_INTERVAL == 0:
                print(f"\n--- Iteration {real_iteration + 1} Evaluation ---")
                
                train_loss, train_dice, train_cls = evaluate_dataset(
                    model, train_loader, seg_criterion, DEVICE,
                    num_samples=len(train_dataset.samples),
                    enable_classification=ENABLE_CLASSIFICATION
                )
                
                val_loss, val_dice, val_cls = evaluate_dataset(
                    model, val_loader, seg_criterion, DEVICE,
                    num_samples=len(val_dataset.samples),
                    enable_classification=ENABLE_CLASSIFICATION
                )
                
                test1_loss, test1_dice, test1_cls = (0.0, 0.0, {})
                test2_loss, test2_dice, test2_cls = (0.0, 0.0, {})
                
                if test1_loader:
                    test1_loss, test1_dice, test1_cls = evaluate_dataset(
                        model, test1_loader, seg_criterion, DEVICE,
                        num_samples=len(test1_dataset.samples),
                        enable_classification=ENABLE_CLASSIFICATION
                    )
                
                if test2_loader:
                    test2_loss, test2_dice, test2_cls = evaluate_dataset(
                        model, test2_loader, seg_criterion, DEVICE,
                        num_samples=len(test2_dataset.samples),
                        enable_classification=ENABLE_CLASSIFICATION
                    )
                
                tracker.add_eval_results(
                    train_loss, train_dice, val_loss, val_dice,
                    test1_loss, test1_dice, test2_loss, test2_dice,
                    train_cls, val_cls, test1_cls, test2_cls
                )
                
                print(f"Train All - Loss: {train_loss:.4f}, Dice: {train_dice:.4f}")
                print(f"Validation - Loss: {val_loss:.4f}, Dice: {val_dice:.4f}")
                if test1_loader:
                    print(f"Test1 - Loss: {test1_loss:.4f}, Dice: {test1_dice:.4f}")
                if test2_loader:
                    print(f"Test2 - Loss: {test2_loss:.4f}, Dice: {test2_dice:.4f}")
                
                # ==================== Model Checkpoint ====================
                current_score = val_dice
                if ENABLE_CLASSIFICATION and val_cls:
                    current_score = val_dice * 0.7 + val_cls.get('f1', 0) * 0.3
                    print(f"Joint Score: Dice({val_dice:.4f}) * 0.7 + F1({val_cls['f1']:.4f}) * 0.3 = {current_score:.4f}")
                
                if current_score > best_dice:
                    best_dice = current_score
                    
                    complete_model_checkpoint = {
                        'model_state_dict': model.state_dict(),
                        'dice_score': val_dice,
                        'iteration': real_iteration + 1,
                        'current_score': current_score,
                        'modalities': MODALITIES,
                        'model_config': {
                            'base_features': BASE_FEATURES,
                            'out_channels': OUT_CHANNELS,
                            'img_size': IMG_SIZE,
                            'enable_classification': ENABLE_CLASSIFICATION
                        }
                    }
                    
                    if ENABLE_CLASSIFICATION and val_cls:
                        complete_model_checkpoint.update({
                            'f1_score': val_cls.get('f1', 0),
                            'accuracy': val_cls.get('accuracy', 0)
                        })
                    
                    torch.save(
                        complete_model_checkpoint,
                        os.path.join(SAVE_DIR, 'best_2d_multimodal_joint_model.pth')
                    )
                    print(f"Saved best complete model! Joint score: {best_dice:.4f}")
                    
                    torch.save(
                        model.state_dict(),
                        os.path.join(SAVE_DIR, 'best_model_weights_only.pth')
                    )
                    print("Saved weights-only version for compatibility")
                else:
                    print("Validation metrics did not improve, continuing training...")
            
            # ==================== Visualization ====================
            if (real_iteration + 1) % VIS_INTERVAL == 0:
                print(f"\n--- Iteration {real_iteration + 1} Visualization ---")
                visualize_multimodal_predictions(
                    model, train_loader, val_loader, test1_loader, test2_loader,
                    tracker, DEVICE, MODALITIES
                )
            
            # ==================== Checkpoint Saving ====================
            if (real_iteration + 1) % SAVE_INTERVAL == 0:
                checkpoint_path = os.path.join(
                    SAVE_DIR,
                    f'2d_multimodal_checkpoint_iter_{real_iteration + 1}.pth'
                )
                save_checkpoint(model, optimizer, tracker, real_iteration + 1, best_dice, checkpoint_path, scheduler)
                tracker.save_curves_to_file(TRAINING_CURVES_FILE)
        
        except Exception as e:
            print(f"Error at iteration {real_iteration + 1}: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    # ==================== Training Completion ====================
    tracker.save_curves_to_file(TRAINING_CURVES_FILE)
    
    if ENABLE_CLASSIFICATION:
        print(f"\n{'=' * 70}")
        print("2D Multimodal Segmentation + Classification Joint Training Complete!")
        print(f"Best Joint Score: {best_dice:.4f}")
        print(f"{'=' * 70}")
    else:
        print(f"\n{'=' * 70}")
        print("2D Multimodal Segmentation Training Complete!")
        print(f"Best Validation Dice: {best_dice:.4f}")
        print(f"{'=' * 70}")


if __name__ == '__main__':
    main()
